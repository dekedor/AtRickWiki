{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and see clusters of Categories among the wikipedia articles using the categorylinks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7524"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file from the repository\n",
    "\n",
    "with open(\"cl_epa.txt\", \"r\") as f:\n",
    "    data=json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I need an array of [[1,2,3... ],[0,3,5...], ...] \n",
    "# so I prepare a vocabulary to then use the vectorizer\n",
    "\n",
    "# Create the all-encompassing vocabulary\n",
    "import re\n",
    "\n",
    "def GetVocabList(data_dict):\n",
    "    #data_dict is a dictionary {keys}:[\"str1\", \"str2\",..., \"strN\"]\n",
    "    #this functions yields [\"str1\", \"str2\", ... \"strM\"], where M is the sum of N over all Keys\n",
    "    list_categories = []\n",
    "    for key in data_dict:  \n",
    "        links=[]\n",
    "        for n in range(len(data_dict[key])):\n",
    "        \n",
    "            # Remove the following regular expressions\n",
    "            \n",
    "            re0=\"protected_page\"\n",
    "            #re1=\"with_accessdate\"\n",
    "            #re2=\".rticle\"\n",
    "            #re3=\".ikipedia\"\n",
    "            #re4=\"to_be_expanded\"\n",
    "            #re5=\"with_unsourced_statements\"\n",
    "            #re6=\"needing_additional_references\"\n",
    "            #re7=\"lacking_sources\"\n",
    "            #re8=\"containing_potentially_dated_statements\"\n",
    "            #re9=\"with_dead_external_links\"\n",
    "            \n",
    "            if not any(re.search(regex, data_dict[key][n]) for regex in [re0]):\n",
    "                links.append(data_dict[key][n])\n",
    "        links=\" \".join(links)\n",
    "        list_categories.append(links)\n",
    "                \n",
    "    return list_categories\n",
    "\n",
    "# list_categories looks like [\"str1 strN\", ..., \"... str137593\"]]. A list strings, each is \" \".join(links_in_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1983_births 21st-century_English_singers All_articles_with_dead_external_links Articles_with_dead_external_links_from_March_2015 Articles_with_hAudio_microformats Articles_with_hCards CS1_Italian-language_sources_(it) CS1_errors:_external_links Commons_category_with_local_link_same_as_on_Wikidata English_mezzo-sopranos English_people_convicted_of_assault English_rhythm_and_blues_singers English_television_personalities EngvarB_from_March_2016 Fascination_Records_artists Footballers'_wives_and_girlfriends Girls_Aloud_members Labour_Party_(UK)_people Living_people Musicians_from_Newcastle_upon_Tyne Official_website_different_in_Wikidata_and_Wikipedia People_convicted_of_assault_occasioning_actual_bodily_harm Popstars_winners Reality_television_judges The_X_Factor_(TV_series)_judges The_X_Factor_(U.S._TV_series) The_X_Factor_(UK_TV_series) Use_dmy_dates_from_March_2016 Wikipedia_articles_with_BNF_identifiers Wikipedia_articles_with_GND_identifiers Wikipedia_articles_with_ISNI_identifiers Wikipedia_articles_with_LCCN_identifiers Wikipedia_articles_with_MusicBrainz_identifiers Wikipedia_articles_with_VIAF_identifiers Wikipedia_indefinitely_semi-protected_biographies_of_living_people\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get clean list of links\n",
    "clean_categories=GetVocabList(data)\n",
    "\n",
    "print(len(clean_categories))\n",
    "clean_categories[3200]   #it's a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_features = vectorizer.fit_transform(clean_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7524, 5000)\n",
      "<class 'numpy.ndarray'>\n",
      "(7524, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_features))\n",
    "print(train_features.shape)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array  (instead of a matrix)\n",
    "train_features_array = train_features.toarray()\n",
    "print(type(train_features_array))\n",
    "print(train_features_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_array[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I need to feed the Isomap an array made of a list of lists (with the vectorized values)!\n",
    "\n",
    "iso = Isomap(n_components=2)\n",
    "data_projected = iso.fit_transform(train_features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50468546, -0.48353731, -0.71127803, ...,  5.18255488,\n",
       "       -0.21534603, -0.30888089])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_projected[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "\n",
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=\"blue\",\n",
    "            edgecolor='none', alpha=0.5);   #, cmap=plt.cm.get_cmap('nipy_spectral', 10)\n",
    "# plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data_projected)):\n",
    "    if data_projected[:1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
