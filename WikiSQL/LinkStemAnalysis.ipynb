{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"cl_epa.txt\", \"r\") as f:\n",
    "    epa_links = json.load(f)\n",
    "with open(\"cl_nepa.txt\", \"r\") as f:\n",
    "    nepa_links = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000s_American_television_series', '2007_American_television_series_debuts', '2008_American_television_series_endings']\n",
      "['2001_establishments_in_Ohio', 'Alive_Naturalsound_Records_artists', 'American_indie_rock_groups']\n"
     ]
    }
   ],
   "source": [
    "print(epa_links[\"5199402\"][:3])\n",
    "print(nepa_links[\"971243\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetWordsLinks(data_dict):\n",
    "    # To get meaningful stemmed category-link words\n",
    "    # It accepts a dictionary {keys}:[\"str1\", \"str2\",..., \"strN\"]\n",
    "    # returns a dataframe where page_id is the index and link_stems is \"stm stm2 stm3 stm4...\"\n",
    "    \n",
    "    list_categories = dict()\n",
    "    for key in data_dict:\n",
    "        links= []\n",
    "        for n in range(len(data_dict[key])):\n",
    "\n",
    "            stop_links = set()\n",
    "            stop_links={\"protected_page\",\"with_accessdate\",\"rticle\",\"ikipedia\",\"to_be_expanded\",\n",
    "                   \"with_unsourced_statements\",\"needing_additional_references\",\"lacking_sources\",\n",
    "                   \"containing_potentially_dated_statements\",\"with_dead_external_links\",\n",
    "                   \"Certification_Table_Entry_usages_for\",\"language_sources\",\"rticles_containing\",\n",
    "                   \"page\",\"CS1\",\"dmy_dates\",\"mdy_dates\",\"Use\",\"use\",\"Wiki\",\"Pages\"} # \"List\",\"list\"\n",
    "            if not any(re.search(regex, data_dict[key][n]) for regex in stop_links):\n",
    "                link = data_dict[key][n]                                 # \"All_articles_in,_Wikipedia...\"\n",
    "                link = re.sub(r\"_\",\" \",link)                             # \"All articles in, Wikipedia\n",
    "                link = re.sub(r\"[^A-Za-z ]\",\"\",link).lower().split()     # [\"all\", \"articles\", \"in\", \"wikipedia\", ... ]\n",
    "                link = [stemmer.stem(w) for w in link if w not in stops] # [\"articl\", \"wikipedia\", ... ]\n",
    "                link = \" \".join(link)                                    # \"articl wikipedia ..\" \n",
    "                links.append(link)\n",
    "        links=\" \".join(links)\n",
    "        list_categories[key]=links\n",
    "    df_links = pd.DataFrame.from_dict(list_categories, orient=\"index\")  # \n",
    "    df_links.index.name = 'article_id'\n",
    "    df_links.columns = ['links_stems']\n",
    "    return df_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_stem_links = GetWordsLinks(epa_links)\n",
    "nepa_stem_links = GetWordsLinks(nepa_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "establish ohio aliv naturalsound record artist american indi rock group american music duo blue rock group brit award winner fat possum record artist grammi award winner music group establish music group akron ohio nonesuch record artist rock music duo suicid squeez record artist v record artist\n",
      "['2001_establishments_in_Ohio', 'Alive_Naturalsound_Records_artists', 'American_indie_rock_groups', 'American_musical_duos', 'Articles_with_hCards', 'Blues_rock_groups', 'Brit_Award_winners', 'CS1_maint:_Multiple_names:_authors_list', 'Fat_Possum_Records_artists', 'Grammy_Award_winners', 'Musical_groups_established_in_2001', 'Musical_groups_from_Akron,_Ohio', 'Nonesuch_Records_artists', 'Rock_music_duos', 'Suicide_Squeeze_Records_artists', 'Use_mdy_dates_from_June_2013', 'V2_Records_artists', 'Wikipedia_articles_with_BNF_identifiers', 'Wikipedia_articles_with_GND_identifiers', 'Wikipedia_articles_with_ISNI_identifiers', 'Wikipedia_articles_with_LCCN_identifiers', 'Wikipedia_articles_with_MusicBrainz_identifiers', 'Wikipedia_articles_with_VIAF_identifiers']\n"
     ]
    }
   ],
   "source": [
    "print(nepa_stem_links[\"links_stems\"][\"971243\"])\n",
    "print(nepa_links[\"971243\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(epa_stem_links[\"links_stems\"][\"307\"])\n",
    "print(nepa_stem_links[\"links_stems\"][\"971243\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def VectorizeMostCommonFeatures(dataframe, num_features):\n",
    "    \"\"\"Identifies the num_features most common features from a dataframe generated by ScrapeAndStemIntros\n",
    "    containing space-delimited stemmed strings.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features = num_features)\n",
    "    vec_words = vectorizer.fit_transform(dataframe[\"links_stems\"])\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    count_vocab = vec_words.toarray()\n",
    "    count_df = pd.DataFrame(count_vocab, columns=vocab, index=dataframe.index)\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10026, 5000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa_link_count = VectorizeMostCommonFeatures(epa_stem_links, 5000)\n",
    "nepa_link_count = VectorizeMostCommonFeatures(nepa_stem_links, 5000)\n",
    "nepa_link_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9897,) (9897, 5000)\n"
     ]
    }
   ],
   "source": [
    "nepa_with_links = nepa_link_count[nepa_link_count.sum(axis=1) > 0].index   # list of ids not empty  7237 of 7524\n",
    "nepa_vecs_with_links = nepa_link_count[nepa_link_count.sum(axis=1) > 0].as_matrix()  # matrix of the articles with links\n",
    "print(nepa_with_links.shape, nepa_vecs_with_links.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9897,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = np.sum(nepa_link_count, axis=0)   # how much a word appears\n",
    "print(counts.shape)\n",
    "nepa_with_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ProcessWordOccurrences(vec_count_df):\n",
    "    '''Given a dataframe (rows: articles, cols: word counts) containg the output of VectorizeMostCommonFeatures, output a dataframe\n",
    "    containing the number of occurrences of each word across the dataset, the number of articles with at least\n",
    "    one occurrence of the word, and the fraction of articles with at least once occurrence.'''\n",
    "    vocab = vec_count_df.columns.values\n",
    "    count_by_word = np.sum(vec_count_df, axis=0)\n",
    "    \n",
    "    present = vec_count_df > np.zeros(vec_count_df.shape)\n",
    "    present_by_word = np.sum(present, axis=0)\n",
    "    present_by_word_frac = present_by_word / vec_count_df.shape[0]\n",
    "    \n",
    "    df = pd.concat([count_by_word,present_by_word,present_by_word_frac],axis=1)\n",
    "    df.columns = ['occurences','articles','frac_of_articles']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_occurrences = ProcessWordOccurrences(epa_link_count)\n",
    "nepa_occurrences = ProcessWordOccurrences(nepa_link_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6687, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ_all = pd.merge(epa_occurrences, nepa_occurrences, how=\"outer\", left_index=True, right_index=True, suffixes=('_epa','_nepa'), indicator=True)\n",
    "occ_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dachau       NaN\n",
       "dagestan     5.0\n",
       "dahl         3.0\n",
       "dai          4.0\n",
       "daiei        NaN\n",
       "daili       11.0\n",
       "dairi        4.0\n",
       "dakota      28.0\n",
       "dal          3.0\n",
       "dalla       45.0\n",
       "Name: occurences_epa, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ_all[\"occurences_epa\"][1540:1550]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================\n",
    "\n",
    "Let's try and get clustering on EPAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['offici', 'ohio', 'oil', 'oiler', 'okinawa', 'oklahoma', 'old',\n",
       "       'oldham', 'olymp', 'olympiaco', 'olympiqu', 'omaha', 'oman', 'one',\n",
       "       'onlin', 'ono', 'ontario', 'open', 'opendomesday', 'oper'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepa_link_count.columns.values[3200:3220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word = \"oak\"\n",
    "# anyid = [art for art in epa_link_count[word].index if epa_link_count[word][str(art)] != 0]\n",
    "# anyid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nepa_norms = np.linalg.norm(nepa_vecs_with_links, axis=1) #calculates norms of row vectors  (7237,)\n",
    "nepa_vecs_normed = nepa_vecs_with_links / nepa_norms[:,None] #unitizes row vectors           (7237,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9897, 5000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepa_vecs_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9897, 9897)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = 180 * np.arccos(np.clip(np.dot(nepa_vecs_normed,nepa_vecs_normed.T),-1.0, 1.0)) / np.pi #calculates the angle\n",
    "#in degrees between each pair of articles      (7237,7237)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9897, 9897)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepa_article_distances = pd.DataFrame(distances, index=nepa_with_links, columns=nepa_with_links) # indexes added\n",
    "nepa_article_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "971243       0.000000\n",
       "193328      36.679215\n",
       "2246167     36.722060\n",
       "3316009     41.407289\n",
       "579441      41.407289\n",
       "2003582     42.186865\n",
       "392096      42.274320\n",
       "453107      43.132955\n",
       "6599787     43.210787\n",
       "28337780    43.671003\n",
       "163883      43.718500\n",
       "Name: 971243, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetMostSimilar(article_id, numrecords=10):\n",
    "    '''Given an article ID, return a list of the most similar (i.e., lowest angle) articles in the dataset'''\n",
    "    return nepa_article_distances[str(article_id)].sort_values()[:numrecords+1]\n",
    "\n",
    "GetMostSimilar(971243, 10)  #971243 for  a nepa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distance scaling function, engineered to increase slowly up to 70 and rapidly thereafter\n",
    "\n",
    "x_plot = np.arange(0,90,0.5)\n",
    "y_plot = (300 + x_plot) / (85 - x_plot) \n",
    "plt.plot(x_plot, y_plot)\n",
    "# plt.xlim((0,70))\n",
    "plt.ylim((0,50))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_distances = (300 + distances) / (85 - distances)\n",
    "scaled_distances[scaled_distances <= 0] = 400 #sets negative values (i.e., angles above 85) to an arbitrarily large distance\n",
    "scaled_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "link_DBSCAN = DBSCAN(eps=45, min_samples=40, metric='precomputed')   # \"scaled_distances\" is the \"precomputed\" metric\n",
    "link_labels = link_DBSCAN.fit_predict(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = len(set(link_labels)) - (1 if -1 in link_labels else 0)\n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, -1}\n",
      "(9897,)\n"
     ]
    }
   ],
   "source": [
    "print(set(link_labels))\n",
    "print(link_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['47159030', '42880738', '2616911', '12880376', '8916716', '42508813',\n",
      "       '309620', '8680627', '720558', '9598063',\n",
      "       ...\n",
      "       '272874', '15290967', '41960962', '23140164', '20789915', '32190061',\n",
      "       '2061626', '39915844', '39803111', '1260189'],\n",
      "      dtype='object', name='article_id', length=9897)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(nepa_with_links)\n",
    "print(nepa_vecs_normed)  # Now I just need to find a way to visualize this: merging the link_labels with epa_vecs_normed \n",
    "# or maybe it's better to visualize the not normed????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CharacterizeClassPop(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), determine how many\n",
    "    non-noise clusters (i.e., some number besides -1) were assigned, what fraction of articles were assigned to a\n",
    "    non-noise cluster, and the average, median, and max size of the population of clusters. Returns a dict.'''\n",
    "    classCounts = np.array(np.unique(topic_labels, return_counts=True)).T #each row contains the class label and the number of occurrences\n",
    "    assignedClasses = classCounts[1:]\n",
    "    numClust = assignedClasses.shape[0]\n",
    "    fracInClust = np.sum(assignedClasses[:,1]) / np.sum(classCounts[:,1])\n",
    "    avgClustSize = np.sum(assignedClasses[:,1]) / numClust\n",
    "    medianClustSize = np.median(assignedClasses[:,1])\n",
    "    maxClustSize = np.max(assignedClasses[:,1])\n",
    "    \n",
    "    return {'numClusters':numClust,\n",
    "           'coverage':fracInClust,\n",
    "           'avgClusterSize':avgClustSize,\n",
    "           'medianClusterSize':medianClustSize,\n",
    "           'maxClusterSize':maxClustSize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DB_class\n",
      "article_id          \n",
      "47159030          -1\n",
      "42880738          -1\n",
      "2616911           -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9897, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(link_labels)\n",
    "id_topic = pd.DataFrame(link_labels, index=nepa_with_links, columns=['DB_class'])\n",
    "print(id_topic[:3])\n",
    "id_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_words_by_class = pd.merge(nepa_link_count, id_topic, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10026, 5000)\n",
      "(9897, 5001)\n",
      "Int64Index([47159030, 42880738,  2616911, 12880376,  8916716, 42508813,\n",
      "              309620,  8680627,   720558,  9598063,\n",
      "            ...\n",
      "              272874, 15290967, 41960962, 23140164, 20789915, 32190061,\n",
      "             2061626, 39915844, 39803111,  1260189],\n",
      "           dtype='int64', length=9897)\n"
     ]
    }
   ],
   "source": [
    "print(nepa_link_count.shape)\n",
    "print(vec_words_by_class.shape)\n",
    "id_topic.index = id_topic.index.map(int)\n",
    "print(id_topic.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -1, 8040],\n",
       "       [   0,  207],\n",
       "       [   1,   62],\n",
       "       [   2,   81],\n",
       "       [   3,  390],\n",
       "       [   4,   96],\n",
       "       [   5,  218],\n",
       "       [   6,  194],\n",
       "       [   7,  104],\n",
       "       [   8,  119],\n",
       "       [   9,   41],\n",
       "       [  10,   56],\n",
       "       [  11,   83],\n",
       "       [  12,   60],\n",
       "       [  13,   54],\n",
       "       [  14,   51],\n",
       "       [  15,   41]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classCount = np.array(np.unique(link_labels, return_counts=True)).T\n",
    "classCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_articles = pd.read_csv(\"070916_edit_protected_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_titles = epa_articles[['page_id','page_title']]\n",
    "epa_titles.index = epa_titles['page_id']\n",
    "del epa_titles['page_id']\n",
    "epa_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_titles.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles_by_class = pd.merge(epa_titles, id_topic, how='inner', left_index=True, right_index=True)\n",
    "titles_by_class[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), select <= 10 representative\n",
    "    articles from each class, and the 10 most frequently present words from that cluster. Returns a nested dict.'''\n",
    "    id_topic = pd.DataFrame(topic_labels, index=nepa_with_links, columns=['DB_class'])\n",
    "    vec_words_by_class = pd.merge(nepa_link_count, id_topic,\n",
    "                                 how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    id_topic.index = id_topic.index.map(int)\n",
    "    #titles_by_class = pd.merge(epa_titles, id_topic,                            # comment this away for nepas\n",
    "    #                           how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    classInfo = dict()\n",
    "    for k in np.unique(topic_labels)[:].tolist():        #  k over the set() of labels (excluded -1)  [array-> list]\n",
    "        classInfo[k] = dict()                             \n",
    "        k_words = vec_words_by_class[vec_words_by_class['DB_class'] == k]  # vectors for the class k\n",
    "        classMem = k_words.shape[0]                                        # number of vec in class k\n",
    "        classInfo[k]['class_members'] = classMem                           # Print-out\n",
    "        present = k_words > np.zeros(k_words.shape)                        # boolean 2D array\n",
    "        present_by_word = np.sum(present, axis=0)                          # sum over class of individual words\n",
    "        present_by_word_frac = present_by_word / k_words.shape[0]          # fraction = occurrences/#class-members\n",
    "        classInfo[k]['top_words'] = present_by_word_frac.sort_values(ascending=False)[:10].to_dict()  # \n",
    "        \n",
    "        sampSize = min(classMem, 10)\n",
    "        #k_titles = titles_by_class[titles_by_class['DB_class'] == k].sample(n=sampSize)  #\n",
    "        #classInfo[k]['rep_titles'] = k_titles['page_title'].tolist()                     # comment these away on nepas\n",
    "    return classInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = InfoAboutEachClass(link_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][\"class_members\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birth: 0.235\n",
      "peopl: 0.226\n",
      "state: 0.132\n",
      "unit: 0.132\n",
      "live: 0.128\n",
      "death: 0.117\n",
      "stub: 0.114\n",
      "establish: 0.113\n",
      "american: 0.111\n",
      "counti: 0.091\n"
     ]
    }
   ],
   "source": [
    "cluster=-1\n",
    "for words in sorted(result[cluster][\"top_words\"], key=result[cluster][\"top_words\"].get, reverse=True):\n",
    "    print(\"{}: {:.3f}\".format(words, result[cluster]['top_words'][words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrintInfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), run InfoAboutEachClass() and format\n",
    "    its output for printing. Returns a formatted string.'''\n",
    "    \n",
    "    printInfo = InfoAboutEachClass(topic_labels)\n",
    "    report = \"\"\n",
    "    for key in sorted(printInfo):\n",
    "        report += \"========================\\n\"\n",
    "        report += \"CLUSTER {}: {} MEMBERS\\n\\n\".format(key, printInfo[key]['class_members'])\n",
    "        \n",
    "        report += \"Representative articles:\\n\"\n",
    "        for article in printInfo[key]['rep_titles']:\n",
    "            report += article\n",
    "            report += \"\\n\"\n",
    "        report += \"\\nMost common terms:\\n\"\n",
    "        for word in sorted(printInfo[key]['top_words'], key=printInfo[key]['top_words'].get, reverse=True):\n",
    "            report += \"{}: {:.3f}\\n\".format(word, printInfo[key]['top_words'][word])\n",
    "        report += \"========================\\n\"\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunAndAnalyzeDBSCAN(eps, min_samples, scaled_distances):\n",
    "    '''Given a value for epsilon, the minimum number of samples required for a core point, and a matrix of\n",
    "    precomputed distances, create a DBSCAN classifier, use it to fit and predict based on the supplied distances, \n",
    "    and analyze the output using CharacterizeClassPop and PrintInfoAboutEachClass. Returns a csv formatted string\n",
    "    containing epsilon, min_samples, numClusters, coverage, avgClusterSize, medianClusterSize, and maxClusterSize'''\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clust = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    topic_labels = clust.fit_predict(scaled_distances)\n",
    "    numClusters = np.unique(topic_labels).shape[0]\n",
    "    if numClusters > 1:\n",
    "        cp = CharacterizeClassPop(topic_labels)\n",
    "        report = PrintInfoAboutEachClass(topic_labels)\n",
    "    \n",
    "        reportFN = \"candidate_clusters/{}_{}_clustReport.txt\".format(eps, min_samples)\n",
    "        with open(reportFN, \"w\") as f:\n",
    "            f.write(report)\n",
    "    \n",
    "        return \"{},{},{},{:.3f},{:.0f},{:.0f},{}\".format(eps,min_samples,cp['numClusters'],cp['coverage'],cp['avgClusterSize'],\n",
    "                                        cp['medianClusterSize'],cp['maxClusterSize'])\n",
    "    else:\n",
    "        return \"{},{},0,0,0,0,0\".format(eps,min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate_eps = range(2,13)\n",
    "candidate_samples = [2,5,10,20,50,100]\n",
    "with open(\"DBSCAN_metrics.txt\",\"w\") as d:\n",
    "    d.write(\"epsilon,min_samples,num_clusters,coverage,avg_cluster_size,median_cluster_size,max_cluster_size\\n\")\n",
    "    for eps in candidate_eps:\n",
    "        for samples in candidate_samples:\n",
    "            output = RunAndAnalyzeDBSCAN(eps, samples, scaled_distances)\n",
    "            d.write(output)\n",
    "            d.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok .. I'm lost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(PrintInfoAboutEachClass(link_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analisi=PrintInfoAboutEachClass(link_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"cl_nepa_noScale_eps40_min40.txt\", \"w\") as f:\n",
    "    f.write(analisi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
