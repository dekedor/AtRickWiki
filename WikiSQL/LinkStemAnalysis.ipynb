{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"cl_epa.txt\", \"r\") as f:\n",
    "    epa_links = json.load(f)\n",
    "with open(\"cl_nepa.txt\", \"r\") as f:\n",
    "    nepa_links = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000s_American_television_series', '2007_American_television_series_debuts', '2008_American_television_series_endings']\n",
      "['2001_establishments_in_Ohio', 'Alive_Naturalsound_Records_artists', 'American_indie_rock_groups']\n"
     ]
    }
   ],
   "source": [
    "print(epa_links[\"5199402\"][:3])\n",
    "print(nepa_links[\"971243\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetWordsLinks(data_dict):\n",
    "    # To get meaningful stemmed category-link words\n",
    "    # It accepts a dictionary {keys}:[\"str1\", \"str2\",..., \"strN\"]\n",
    "    # returns a dataframe where page_id is the index and link_stems is \"stm stm2 stm3 stm4...\"\n",
    "    \n",
    "    list_categories = dict()\n",
    "    for key in data_dict:\n",
    "        links= []\n",
    "        for n in range(len(data_dict[key])):\n",
    "\n",
    "            stop_links = set()\n",
    "            stop_links={\"protected_page\",\"with_accessdate\",\"rticle\",\"ikipedia\",\"to_be_expanded\",\n",
    "                   \"with_unsourced_statements\",\"needing_additional_references\",\"lacking_sources\",\n",
    "                   \"containing_potentially_dated_statements\",\"with_dead_external_links\",\n",
    "                   \"Certification_Table_Entry_usages_for\",\"language_sources\",\"rticles_containing\",\n",
    "                   \"page\",\"CS1\",\"mdy_dates\",\"Wiki\",\"Pages\",\"List\",\"list\"}\n",
    "            if not any(re.search(regex, data_dict[key][n]) for regex in stop_links):\n",
    "                link = data_dict[key][n]                                 # \"All_articles_in,_Wikipedia...\"\n",
    "                link = re.sub(r\"_\",\" \",link)                             # \"All articles in, Wikipedia\n",
    "                link = re.sub(r\"[^A-Za-z ]\",\"\",link).lower().split()     # [\"all\", \"articles\", \"in\", \"wikipedia\", ... ]\n",
    "                link = [stemmer.stem(w) for w in link if w not in stops] # [\"articl\", \"wikipedia\", ... ]\n",
    "                link = \" \".join(link)                                    # \"articl wikipedia ..\" \n",
    "                links.append(link)\n",
    "        links=\" \".join(links)\n",
    "        list_categories[key]=links\n",
    "    df_links = pd.DataFrame.from_dict(list_categories, orient=\"index\")  # \n",
    "    df_links.index.name = 'article_id'\n",
    "    df_links.columns = ['links_stems']\n",
    "    return df_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_stem_links = GetWordsLinks(epa_links)\n",
    "nepa_stem_links = GetWordsLinks(nepa_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction american peopl english descent fiction adopte fiction charact introduc fiction child sexual abus victim fiction murder fiction newspap publish fiction rapist fiction report fiction twin gener hospit charact one life live charact\n",
      "['Featured_articles', 'Fictional_American_people_of_English_descent', 'Fictional_adoptees', 'Fictional_characters_introduced_in_1992', 'Fictional_child_sexual_abuse_victims', 'Fictional_murderers', 'Fictional_newspaper_publishers', 'Fictional_rapists', 'Fictional_reporters', 'Fictional_twins', 'General_Hospital_characters', 'One_Life_to_Live_characters', 'Wikipedia_pages_semi-protected_from_banned_users']\n"
     ]
    }
   ],
   "source": [
    "print(epa_stem_links[\"links_stems\"][\"713342\"])\n",
    "print(epa_links[\"713342\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birth death thcenturi american politician thcenturi christian ac element abraham lincoln american classic liber american peopl english descent american postmast assassin presid unit state assassin head state burial oak ridg cemeteri death firearm washington dc hall fame great american inducte illinoi republican illinoi whig illinoi lawyer lincoln famili member illinoi hous repres member unit state hous repres illinoi peopl cole counti illinoi peopl laru counti kentucki peopl macon counti illinoi peopl spencer counti indiana peopl murder washington dc peopl illinoi american civil war peopl mood disord polit parti founder politician springfield illinoi presid unit state republican parti unit state presidenti nomine republican parti presid unit state smallpox survivor union polit leader unit state presidenti candid unit state presidenti candid use mdi date januari whig parti member unit state hous repres\n",
      "establish ohio aliv naturalsound record artist american indi rock group american music duo blue rock group brit award winner fat possum record artist grammi award winner music group establish music group akron ohio nonesuch record artist rock music duo suicid squeez record artist use mdi date june v record artist\n"
     ]
    }
   ],
   "source": [
    "print(epa_stem_links[\"links_stems\"][\"307\"])\n",
    "print(nepa_stem_links[\"links_stems\"][\"971243\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def VectorizeMostCommonFeatures(dataframe, num_features):\n",
    "    \"\"\"Identifies the num_features most common features from a dataframe generated by ScrapeAndStemIntros\n",
    "    containing space-delimited stemmed strings.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features = num_features)\n",
    "    vec_words = vectorizer.fit_transform(dataframe[\"links_stems\"])\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    count_vocab = vec_words.toarray()\n",
    "    count_df = pd.DataFrame(count_vocab, columns=vocab, index=dataframe.index)\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_link_count = VectorizeMostCommonFeatures(epa_stem_links, 5000)\n",
    "nepa_link_count = VectorizeMostCommonFeatures(nepa_stem_links, 5000)\n",
    "epa_link_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_with_links = epa_link_count[epa_link_count.sum(axis=1) > 0].index   # list of ids not empty  7237 of 7524\n",
    "epa_vecs_with_links = epa_link_count[epa_link_count.sum(axis=1) > 0].as_matrix()  # matrix of the articles with links\n",
    "print(epa_with_links.shape, epa_vecs_with_links.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = np.sum(epa_link_count, axis=0)   # how much a word appears\n",
    "print(counts.shape)\n",
    "epa_with_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ProcessWordOccurrences(vec_count_df):\n",
    "    '''Given a dataframe (rows: articles, cols: word counts) containg the output of VectorizeMostCommonFeatures, output a dataframe\n",
    "    containing the number of occurrences of each word across the dataset, the number of articles with at least\n",
    "    one occurrence of the word, and the fraction of articles with at least once occurrence.'''\n",
    "    vocab = vec_count_df.columns.values\n",
    "    count_by_word = np.sum(vec_count_df, axis=0)\n",
    "    \n",
    "    present = vec_count_df > np.zeros(vec_count_df.shape)\n",
    "    present_by_word = np.sum(present, axis=0)\n",
    "    present_by_word_frac = present_by_word / vec_count_df.shape[0]\n",
    "    \n",
    "    df = pd.concat([count_by_word,present_by_word,present_by_word_frac],axis=1)\n",
    "    df.columns = ['occurences','articles','frac_of_articles']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_occurrences = ProcessWordOccurrences(epa_link_count)\n",
    "nepa_occurrences = ProcessWordOccurrences(nepa_link_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occ_all = pd.merge(epa_occurrences, nepa_occurrences, how=\"outer\", left_index=True, right_index=True, suffixes=('_epa','_nepa'), indicator=True)\n",
    "occ_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occ_all[\"occurences_epa\"][1540:1550]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================\n",
    "\n",
    "Let's try and get clustering on EPAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_link_count.columns.values[3200:3220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word = \"oak\"\n",
    "# anyid = [art for art in epa_link_count[word].index if epa_link_count[word][str(art)] != 0]\n",
    "# anyid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epa_norms = np.linalg.norm(epa_vecs_with_links, axis=1) #calculates norms of row vectors  (7237,)\n",
    "epa_vecs_normed = epa_vecs_with_links / epa_norms[:,None] #unitizes row vectors           (7237,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_vecs_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances = 180 * np.arccos(np.clip(np.dot(epa_vecs_normed,epa_vecs_normed.T),-1.0, 1.0)) / np.pi #calculates the angle\n",
    "#in degrees between each pair of articles      (7237,7237)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_article_distances = pd.DataFrame(distances, index=epa_with_links, columns=epa_with_links) # indexes added\n",
    "epa_article_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetMostSimilar(article_id, numrecords=10):\n",
    "    '''Given an article ID, return a list of the most similar (i.e., lowest angle) articles in the dataset'''\n",
    "    return epa_article_distances[str(article_id)].sort_values()[:numrecords+1]\n",
    "\n",
    "GetMostSimilar(25, 10)  #971243 for  a nepa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distance scaling function, engineered to increase slowly up to 70 and rapidly thereafter\n",
    "\n",
    "x_plot = np.arange(0,90,0.5)\n",
    "y_plot = (300 + x_plot) / (85 - x_plot) \n",
    "plt.plot(x_plot, y_plot)\n",
    "# plt.xlim((0,70))\n",
    "plt.ylim((0,50))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_distances = (300 + distances) / (85 - distances)\n",
    "scaled_distances[scaled_distances <= 0] = 400 #sets negative values (i.e., angles above 85) to an arbitrarily large distance\n",
    "scaled_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "link_DBSCAN = DBSCAN(eps=40, min_samples=40, metric='precomputed')   # \"scaled_distances\" is the \"precomputed\" metric\n",
    "link_labels = link_DBSCAN.fit_predict(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = len(set(link_labels)) - (1 if -1 in link_labels else 0)\n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(set(link_labels))\n",
    "print(link_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(epa_with_links)\n",
    "print(epa_vecs_normed)  # Now I just need to find a way to visualize this: merging the link_labels with epa_vecs_normed \n",
    "# or maybe it's better to visualize the not normed????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CharacterizeClassPop(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), determine how many\n",
    "    non-noise clusters (i.e., some number besides -1) were assigned, what fraction of articles were assigned to a\n",
    "    non-noise cluster, and the average, median, and max size of the population of clusters. Returns a dict.'''\n",
    "    classCounts = np.array(np.unique(topic_labels, return_counts=True)).T #each row contains the class label and the number of occurrences\n",
    "    assignedClasses = classCounts[1:]\n",
    "    numClust = assignedClasses.shape[0]\n",
    "    fracInClust = np.sum(assignedClasses[:,1]) / np.sum(classCounts[:,1])\n",
    "    avgClustSize = np.sum(assignedClasses[:,1]) / numClust\n",
    "    medianClustSize = np.median(assignedClasses[:,1])\n",
    "    maxClustSize = np.max(assignedClasses[:,1])\n",
    "    \n",
    "    return {'numClusters':numClust,\n",
    "           'coverage':fracInClust,\n",
    "           'avgClusterSize':avgClustSize,\n",
    "           'medianClusterSize':medianClustSize,\n",
    "           'maxClusterSize':maxClustSize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(link_labels)\n",
    "id_topic = pd.DataFrame(link_labels, index=epa_with_links, columns=['DB_class'])\n",
    "print(id_topic[:3])\n",
    "id_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_words_by_class = pd.merge(epa_link_count, id_topic, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(epa_link_count.shape)\n",
    "print(vec_words_by_class.shape)\n",
    "id_topic.index = id_topic.index.map(int)\n",
    "print(id_topic.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classCount = np.array(np.unique(link_labels, return_counts=True)).T\n",
    "classCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_articles = pd.read_csv(\"070916_edit_protected_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_titles = epa_articles[['page_id','page_title']]\n",
    "epa_titles.index = epa_titles['page_id']\n",
    "del epa_titles['page_id']\n",
    "epa_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_titles.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles_by_class = pd.merge(epa_titles, id_topic, how='inner', left_index=True, right_index=True)\n",
    "titles_by_class[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), select <= 10 representative\n",
    "    articles from each class, and the 10 most frequently present words from that cluster. Returns a nested dict.'''\n",
    "    id_topic = pd.DataFrame(topic_labels, index=epa_with_links, columns=['DB_class'])\n",
    "    vec_words_by_class = pd.merge(epa_link_count, id_topic,\n",
    "                                 how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    id_topic.index = id_topic.index.map(int)\n",
    "    titles_by_class = pd.merge(epa_titles, id_topic,                            # comment this away for nepas\n",
    "                               how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    classInfo = dict()\n",
    "    for k in np.unique(topic_labels)[1:].tolist():        #  k over the set() of labels (excluded -1)  [array-> list]\n",
    "        classInfo[k] = dict()                             \n",
    "        k_words = vec_words_by_class[vec_words_by_class['DB_class'] == k]  # vectors for the class k\n",
    "        classMem = k_words.shape[0]                                        # number of vec in class k\n",
    "        classInfo[k]['class_members'] = classMem                           # Print-out\n",
    "        present = k_words > np.zeros(k_words.shape)                        # boolean 2D array\n",
    "        present_by_word = np.sum(present, axis=0)                          # sum over class of individual words\n",
    "        present_by_word_frac = present_by_word / k_words.shape[0]          # fraction = occurrences/#class-members\n",
    "        classInfo[k]['top_words'] = present_by_word_frac.sort_values(ascending=False)[:20].to_dict()  # \n",
    "        \n",
    "        sampSize = min(classMem, 10)\n",
    "        k_titles = titles_by_class[titles_by_class['DB_class'] == k].sample(n=sampSize)  #\n",
    "        classInfo[k]['rep_titles'] = k_titles['page_title'].tolist()                     # comment these away on nepas\n",
    "    return classInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = InfoAboutEachClass(link_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result[8][\"class_members\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result[8][\"top_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrintInfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), run InfoAboutEachClass() and format\n",
    "    its output for printing. Returns a formatted string.'''\n",
    "    \n",
    "    printInfo = InfoAboutEachClass(topic_labels)\n",
    "    report = \"\"\n",
    "    for key in sorted(printInfo):\n",
    "        report += \"========================\\n\"\n",
    "        report += \"CLUSTER {}: {} MEMBERS\\n\\n\".format(key, printInfo[key]['class_members'])\n",
    "        \n",
    "        report += \"Representative articles:\\n\"\n",
    "        for article in printInfo[key]['rep_titles']:\n",
    "            report += article\n",
    "            report += \"\\n\"\n",
    "        report += \"\\nMost common terms:\\n\"\n",
    "        for word in sorted(printInfo[key]['top_words'], key=printInfo[key]['top_words'].get, reverse=True):\n",
    "            report += \"{}: {:.3f}\\n\".format(word, printInfo[key]['top_words'][word])\n",
    "        report += \"========================\\n\"\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunAndAnalyzeDBSCAN(eps, min_samples, scaled_distances):\n",
    "    '''Given a value for epsilon, the minimum number of samples required for a core point, and a matrix of\n",
    "    precomputed distances, create a DBSCAN classifier, use it to fit and predict based on the supplied distances, \n",
    "    and analyze the output using CharacterizeClassPop and PrintInfoAboutEachClass. Returns a csv formatted string\n",
    "    containing epsilon, min_samples, numClusters, coverage, avgClusterSize, medianClusterSize, and maxClusterSize'''\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clust = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    topic_labels = clust.fit_predict(scaled_distances)\n",
    "    numClusters = np.unique(topic_labels).shape[0]\n",
    "    if numClusters > 1:\n",
    "        cp = CharacterizeClassPop(topic_labels)\n",
    "        report = PrintInfoAboutEachClass(topic_labels)\n",
    "    \n",
    "        reportFN = \"candidate_clusters/{}_{}_clustReport.txt\".format(eps, min_samples)\n",
    "        with open(reportFN, \"w\") as f:\n",
    "            f.write(report)\n",
    "    \n",
    "        return \"{},{},{},{:.3f},{:.0f},{:.0f},{}\".format(eps,min_samples,cp['numClusters'],cp['coverage'],cp['avgClusterSize'],\n",
    "                                        cp['medianClusterSize'],cp['maxClusterSize'])\n",
    "    else:\n",
    "        return \"{},{},0,0,0,0,0\".format(eps,min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate_eps = range(2,13)\n",
    "candidate_samples = [2,5,10,20,50,100]\n",
    "with open(\"DBSCAN_metrics.txt\",\"w\") as d:\n",
    "    d.write(\"epsilon,min_samples,num_clusters,coverage,avg_cluster_size,median_cluster_size,max_cluster_size\\n\")\n",
    "    for eps in candidate_eps:\n",
    "        for samples in candidate_samples:\n",
    "            output = RunAndAnalyzeDBSCAN(eps, samples, scaled_distances)\n",
    "            d.write(output)\n",
    "            d.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok .. I'm lost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(PrintInfoAboutEachClass(link_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analisi=PrintInfoAboutEachClass(link_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"cl_nepa_noScale_eps40_min40.txt\", \"w\") as f:\n",
    "    f.write(analisi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
