{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you learn from this:\n",
    "\n",
    "There are 142436 category links in the epa database, of which:\n",
    "-  45473 category links that include \"wikipedia\", \"article\", \"with_accessdate\" and \"protected_page\" \n",
    "-  96963 category links that NOT include them\n",
    "-  4843 category links with the expression \"protected_page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file in the repository\n",
    "\n",
    "with open(\"cl_epa.txt\", \"r\") as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7524"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the all-encompassing list of categories that will train the algorithm\n",
    "import re\n",
    "\n",
    "def GetVocabClean(data):\n",
    "    #data is a dictionary {keys}:[\"str1\", \"str2\",..., \"strN\"]\n",
    "    #this functions yields [\"str1\", \"str2\", ... \"strM\"], where M is the sum of N over all Keys\n",
    "    list_categories = []\n",
    "    for key in data:    \n",
    "        for n in range(len(data[key])):\n",
    "        \n",
    "            # Remove all references to %dmy_dates_from_%\", \"%rticle%\", \"%ikipedia%\", and \"%with_accessdate%\"\n",
    "            \n",
    "            re0=\"protected_page\"\n",
    "            #re1=\"with_accessdate\"\n",
    "            #re2=\".rticle\"\n",
    "            #re3=\".ikipedia\"\n",
    "            #re4=\"to_be_expanded\"\n",
    "            #re5=\"with_unsourced_statements\"\n",
    "            #re6=\"needing_additional_references\"\n",
    "            #re7=\"lacking_sources\"\n",
    "            #re8=\"containing_potentially_dated_statements\"\n",
    "            #re9=\"with_dead_external_links\"\n",
    "            \n",
    "            \n",
    "            if not any(re.search(regex, data[key][n]) for regex in [re0]):\n",
    "                list_categories.append(data[key][n])\n",
    "                \n",
    "    return list_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Video_games_developed_in_the_United_States',\n",
       " 'Video_games_featuring_anthropomorphic_characters',\n",
       " 'Windows_Mobile_Professional_games',\n",
       " 'Windows_Mobile_Standard_games',\n",
       " 'Windows_games',\n",
       " 'Xbox_360_Live_Arcade_games',\n",
       " 'Zeebo_games',\n",
       " '1942_establishments_in_the_United_States',\n",
       " '1946_disestablishments_in_the_United_States',\n",
       " 'Atomic_bombings_of_Hiroshima_and_Nagasaki']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get clean list of links\n",
    "clean_categories=GetVocabClean(data)\n",
    "\n",
    "print(len(clean_categories))\n",
    "clean_categories[3200:3210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(train_data_features))\n",
    "print(train_data_features.shape)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array  (instead of a matrix)\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(type(train_data_features))\n",
    "train_data_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "# I should look at all the categories that contain \"articles\" and sift through them!\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab[1200:1220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for i,j in zip(vocab, dist):\n",
    "    if j>500:\n",
    "        print(j,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist, bins = np.histogram(dist,bins=np.logspace(1.4, 3.2, 50), range=(0,4000), density=False)\n",
    "hist[7]    #around bin#7 I have hit 500 less common categories, that corresponds to about an occurrency of 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the histogram\n",
    "\n",
    "%matplotlib inline \n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.ylabel('CategoryLinks')\n",
    "plt.xlabel('Occurrencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", data_dic['307'][2] ) #this is a way to get only letters and remove simbols and underscore\n",
    "print(letters_only)\n",
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words\n",
    "print(words[:20])\n",
    "merge=\" \".join(words)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e luce fu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex_str1=\"wiki\"\n",
    "regex_str2=\".rti\"\n",
    "line=\"Wikipedia_articles\"\n",
    "risultato=any(re.search(regex, line) for regex in [regex_str1, regex_str2])\n",
    "if risultato:\n",
    "    print(\"e luce fu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string=\"Articole_in_wikipedia\"\n",
    "risultato=bool(re.search(\".ikipedia\", string))\n",
    "risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
