{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you learn from this:\n",
    "\n",
    "There are 142436 category links in the epa database, of which:\n",
    "-  45473 category links that include \"wikipedia\", \"article\", \"with_accessdate\" and \"protected_page\" \n",
    "-  96963 category links that NOT include them\n",
    "-  4843 category links with the expression \"protected_page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7524\n"
     ]
    }
   ],
   "source": [
    "# Open the file in the repository\n",
    "\n",
    "with open(\"cl_epa.txt\", \"r\") as f:\n",
    "    data=json.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the all-encompassing list of categories that will train the random forest\n",
    "import re\n",
    "\n",
    "def GetVocabList(data_dict):\n",
    "    #data_dict is a dictionary {keys}:[\"str1\", \"str2\",..., \"strN\"]\n",
    "    #this functions yields [\"str1\", \"str2\", ... \"strM\"], where M is the sum of N over all Keys\n",
    "    list_categories = []\n",
    "    for key in data_dict:  \n",
    "        links=[]\n",
    "        for n in range(len(data_dict[key])):\n",
    "        \n",
    "            # Remove the following regular expressions\n",
    "            \n",
    "            re0=\"protected_page\"\n",
    "            #re1=\"with_accessdate\"\n",
    "            #re2=\".rticle\"\n",
    "            #re3=\".ikipedia\"\n",
    "            #re4=\"to_be_expanded\"\n",
    "            #re5=\"with_unsourced_statements\"\n",
    "            #re6=\"needing_additional_references\"\n",
    "            #re7=\"lacking_sources\"\n",
    "            #re8=\"containing_potentially_dated_statements\"\n",
    "            #re9=\"with_dead_external_links\"\n",
    "            \n",
    "            if not any(re.search(regex, data_dict[key][n]) for regex in [re0]):\n",
    "                links.append(data_dict[key][n])\n",
    "        links=\" \".join(links)\n",
    "        list_categories.append(links)\n",
    "                \n",
    "    return list_categories\n",
    "\n",
    "# list_categories looks like [[\"str1\",...],[\"strN\", ...],...[...,\"str137593\"]]. A list of list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['All_articles_with_unsourced_statements Articles_containing_Hebrew-language_text Articles_with_unsourced_statements_from_June_2016 Castles_in_Syria Commons_category_with_local_link_same_as_on_Wikidata Crusades Forts_in_Syria Mameluk_castles Medieval_sites_on_the_Golan_Heights National_parks_of_Israel Tourist_attractions_in_Israeli-occupied_territories',\n",
       " '1992_births 21st-century_American_singers African-American_male_singers African-American_singer-songwriters American_hip_hop_singers American_rhythm_and_blues_singer-songwriters American_soul_singers Articles_with_hCards Def_Jam_Recordings_artists Living_people R&B_musicians_from_New_Orleans Southern_hip_hop_musicians Wikipedia_articles_with_MusicBrainz_identifiers']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get clean list of links\n",
    "clean_categories=GetVocabList(data)\n",
    "\n",
    "print(len(clean_categories))\n",
    "clean_categories[:2]\n",
    "\n",
    "#clean_categories looks like [[\"str1\",...],[\"strN\", ...],...[...,\"str137593\"]]. A list of list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_features = vectorizer.fit_transform(clean_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7524, 5000)\n",
      "<class 'numpy.ndarray'>\n",
      "(7524, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_features))\n",
    "print(train_features.shape)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array  (instead of a matrix)\n",
    "train_features_array = train_features.toarray()\n",
    "print(type(train_features_array))\n",
    "print(train_features_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_array[2][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american_soul_singers', 'american_spiritual_writers', 'american_sport_wrestlers', 'american_sports_businesspeople', 'american_sports_executives_and_administrators', 'american_sports_films', 'american_sports_radio_personalities', 'american_sportspeople_in_doping_cases', 'american_sportswriters', 'american_stage_actresses', 'american_stand', 'american_stock_traders', 'american_suffragists', 'american_synthpop_musicians', 'american_taekwondo_practitioners', 'american_talent_agents', 'american_talk_radio_hosts', 'american_technology_chief_executives', 'american_technology_company_founders', 'american_television_actresses', 'american_television_directors', 'american_television_evangelists', 'american_television_films', 'american_television_hosts', 'american_television_news_anchors', 'american_television_personalities', 'american_television_producers', 'american_television_reporters_and_correspondents', 'american_television_sitcoms', 'american_television_sports_announcers', 'american_television_talk_show_hosts', 'american_television_writers', 'american_tenors', 'american_unitarians', 'american_video_game_actresses', 'american_video_game_designers', 'american_voice_actresses', 'american_war', 'american_war_correspondents', 'american_websites', 'american_women_activists', 'american_women_business_executives', 'american_women_chief_executives', 'american_women_comedians', 'american_women_film_directors', 'american_women_in_business', 'american_women_in_politics', 'american_women_journalists', 'american_women_lawyers', 'american_women_novelists']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "# I should look at all the categories that contain \"articles\" and sift through them!\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for i,j in zip(vocab, dist):\n",
    "    if j>500:\n",
    "        print(j,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist, bins = np.histogram(dist,bins=np.logspace(1.4, 3.2, 50), range=(0,4000), density=False)\n",
    "hist[7]    #around bin#7 I have hit 500 less common categories, that corresponds to about an occurrency of 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the histogram\n",
    "\n",
    "%matplotlib inline \n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.ylabel('CategoryLinks')\n",
    "plt.xlabel('Occurrencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", data_dic['307'][2] ) #this is a way to get only letters and remove simbols and underscore\n",
    "print(letters_only)\n",
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words\n",
    "print(words[:20])\n",
    "merge=\" \".join(words)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex_str1=\"wiki\"\n",
    "regex_str2=\".rti\"\n",
    "line=\"Wikipedia_articles\"\n",
    "risultato=any(re.search(regex, line) for regex in [regex_str1, regex_str2])\n",
    "if risultato:\n",
    "    print(\"e luce fu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "string=\"Articole_in_wikipedia\"\n",
    "risultato=bool(re.search(\".ikipedia\", string))\n",
    "risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
