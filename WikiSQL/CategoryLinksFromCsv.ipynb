{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First connect through SSH tunnel to tools-login.wmflabs.org server.\n",
    "\n",
    "Replace the username and password (below) with those you find in the replica.my.cnf file in the host home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql                                               # to get, do pip install PyMySQL\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #from the NLP tutorial\n",
    "import re                                                    #from NLP tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This connects to the Wiki database\n",
    "\n",
    "username = 'u15068'                # get from replica.my.cnf\n",
    "password = 'G5V2xrWbdFTSkyoU'      # get from replica.my.cnf\n",
    "db=pymysql.connect(host='localhost',port=4711,user=username,passwd=password,db='enwiki_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7526, 20) 7526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300    11857\n",
       "301    11867\n",
       "302    11921\n",
       "303    11955\n",
       "304    11968\n",
       "Name: page_id, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the epa id numbers from the dataset on github\n",
    "\n",
    "epa_articles = pd.read_csv(\"070916_edit_protected_articles.csv\")\n",
    "print(epa_articles.shape, len(epa_articles))\n",
    "epa_ids=epa_articles[\"page_id\"]\n",
    "epa_ids[300:305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION to Quary the wiki database for caterogylinks a page_id at a time\n",
    "# To use it you need to be connected to a database through PyMySQL\n",
    "\n",
    "# The result is a dictionary in the format {\"page_id\" : [\"link1\", \"link2\", ...]}\n",
    "\n",
    "def GetDictWikiLinks(database, art_ids):\n",
    "    data={}\n",
    "    for i in range(len(art_ids)):       #to get them all len(epa_articles)\n",
    "        cur = database.cursor()               #this is how you use pymusql apparently\n",
    "        if i %1000 ==0:                 #progress print statement\n",
    "            print(\"Quarry article\", art_ids[i])\n",
    "    \n",
    "        #########################_____SQL_____##############################\n",
    "        # This is the SQL command to quarry the Wikipedia database         #\n",
    "        # The syntax of a quarry is:                                       #\n",
    "        # SELECT [columns] FROM [table] WHERE [conditions] LIMIT [optional]#\n",
    "        \n",
    "        quarry=\"\"\"\n",
    "        SELECT pl_from, pl_title\n",
    "        FROM pagelinks\n",
    "        WHERE pl_from = %s\"\"\"\n",
    "        cur.execute(quarry, np.asscalar(art_ids[i]))\n",
    "        links = np.array(cur.fetchall())\n",
    "    \n",
    "        #Some quarries might be empty \n",
    "        if links.size is 0:\n",
    "            print(\"Empty categorylink %s\", art_ids[i])\n",
    "        else:   \n",
    "            key=links[0,0].decode(\"utf-8\")\n",
    "            data[key]=[links[l,1].decode(\"utf-8\") for l in range(len(links[:,1]))]\n",
    "    \n",
    "        #might take about 10 minuts (10k quarries, or 30k links)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarry article 25\n",
      "Quarry article 39988\n",
      "Quarry article 326123\n",
      "Quarry article 1331018\n",
      "Empty categorylink %s 4485200\n",
      "Quarry article 4553108\n",
      "Quarry article 13253481\n",
      "Quarry article 24572232\n",
      "Quarry article 41698932\n"
     ]
    }
   ],
   "source": [
    "# Get the dictionary from Wikipedia database\n",
    "\n",
    "data=GetDictWikiLinks(db, epa_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1782',\n",
       " '1862',\n",
       " '1931',\n",
       " '1931_Constitution_of_Ethiopia',\n",
       " '1990',\n",
       " '1990_Luzon_earthquake',\n",
       " '1st_Cavalry_Division_Horse_Cavalry_Detachment',\n",
       " '2002_Atlantic_hurricane_season',\n",
       " '2008',\n",
       " '2008_Chinese_milk_scandal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"15580374\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write data as a JSON to file\n",
    "\n",
    "with open('pl_epa.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        # Examples of querries!\n",
    "        \n",
    "        # to get the category links\n",
    "        # SELECT cl_from, cl_to\n",
    "        # FROM categorylinks\n",
    "        # WHERE cl_from = %s\n",
    "        \n",
    "        # To get the page-links\n",
    "        # SELECT pl_from, pl_title\n",
    "        # FROM pagelinks\n",
    "        # WHERE pl_from = %s\n",
    "        # AND pl_namespace = 0\n",
    "        \n",
    "        # to get stuff from different tables:\n",
    "        # SELECT page_id, cl_to\n",
    "        # FROM page inner join categorylinks on(page.page_id = categorylinks.cl_from)\n",
    "        # WHERE page_id = %s\n",
    "        # actually that could have been simplyfied, but it nicely shows how to merge tables with \"inner join\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#then I would do the same thing, but parsing the words from the categories separately\n",
    "#Then the same thing but including all the links in the article from the other table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep the really specific topics that occur above some threshold of pages (5%? 10%?), then split the topics into words, filter stop words, and the bag of words to get things that are common enough to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I should try and run my own random forest, but first\n",
    "#I should get a sample of non edit-protected articles (what if they have other kinds of protection?)\n",
    "#then mix them with my dataset, make sure they are labelled, split in training and validation sets \n",
    "#train a random forest algorithm\n",
    "#To do that I will have to modify the query and I might want to branch off this notebook, so first I should push it on github \n",
    "#And this is a good time to re-practice that"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
