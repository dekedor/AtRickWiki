{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First connect through SSH tunnel to tools-login.wmflabs.org server.\n",
    "\n",
    "Replace the username and password (below) with those you find in the replica.my.cnf file in the host home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql                                               # to get, do pip install PyMySQL\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #from the NLP tutorial\n",
    "import re                                                    #from NLP tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This connects to the Wiki database\n",
    "\n",
    "username = 'u15068'                # get from replica.my.cnf\n",
    "password = 'G5V2xrWbdFTSkyoU'      # get from replica.my.cnf\n",
    "db=pymysql.connect(host='localhost',port=4711,user=username,passwd=password,db='enwiki_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7526, 20) 7526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "628"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the epa id numbers from the dataset on github\n",
    "\n",
    "epa_articles = pd.read_csv(\"070916_edit_protected_articles.csv\")\n",
    "print(epa_articles.shape, len(epa_articles))\n",
    "epa_ids=epa_articles[\"page_id\"]\n",
    "epa_ids[300:305]\n",
    "epa_ids[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarry article 25\n",
      "Quarry article 39988\n",
      "Quarry article 326123\n",
      "Quarry article 1331018\n",
      "Empty categorylink %s 4485200\n",
      "Quarry article 4553108\n",
      "Quarry article 13253481\n",
      "Quarry article 24572232\n",
      "Quarry article 41698932\n",
      "Empty categorylink %s 15580374\n"
     ]
    }
   ],
   "source": [
    "#Quary the wiki db for caterogylinks a page_id at a time\n",
    "#creates the dictionary data, in the format {\"page_id\" : [\"link1\", \"link2\", ...]} for all edit-protected articles\n",
    "\n",
    "data={}\n",
    "for i in range(len(epa_ids)):       #to get them all len(epa_articles)\n",
    "    cur = db.cursor()                  #this is how you use pymusql apparently\n",
    "    page_id=epa_ids[i]\n",
    "    if i %1000 ==0:\n",
    "        print(\"Quarry article\", epa_ids[i])\n",
    "    quarry=\"\"\"\n",
    "    SELECT page_id, cl_to\n",
    "    FROM page inner join categorylinks on(page.page_id = categorylinks.cl_from)\n",
    "    WHERE page_id = %s\"\"\" \n",
    "    cur.execute(quarry, np.asscalar(epa_ids[i]))\n",
    "    link = np.array(cur.fetchall())\n",
    "    if link.size is 0:\n",
    "        print(\"Empty categorylink %s\", epa_ids[i])\n",
    "    else:   \n",
    "        key=link[0,0].decode(\"utf-8\")\n",
    "        data[key]=[link[l,1].decode(\"utf-8\") for l in range(len(link[:,1]))]\n",
    "    \n",
    "    #link = np.array(cur.fetchall())\n",
    "    #data.append(link.tolist())\n",
    "    #data=np.concatenate((data,link), axis=0)\n",
    "    #Takes 8minuts to 10 min\n",
    "    #link[:,1].tolist() this is to send an array in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7524"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'50730392', b'1990_births'],\n",
       "       [b'50730392', b'All_articles_needing_additional_references'],\n",
       "       [b'50730392', b'Articles_for_deletion'],\n",
       "       [b'50730392',\n",
       "        b'Articles_needing_additional_references_from_June_2016'],\n",
       "       [b'50730392', b'Articles_to_be_expanded_from_June_2016'],\n",
       "       [b'50730392', b'Astralis_players'],\n",
       "       [b'50730392', b'Counter-Strike_players'],\n",
       "       [b'50730392', b'Danish_eSports_players'],\n",
       "       [b'50730392', b'Fnatic_players'],\n",
       "       [b'50730392', b'Living_people'],\n",
       "       [b'50730392', b'Mousesports_players'],\n",
       "       [b'50730392', b'Team_Dignitas_players'],\n",
       "       [b'50730392', b'Team_SoloMid_players']], \n",
       "      dtype='|S53')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('cl_epa.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Bag of words tool needs a list of words to be trained on, so I extract that from the dictionary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I need to check that this works on the datashape I produce\n",
    "clean_train_categories = []\n",
    "for key in data:                                                               \n",
    "    clean_train_categories.append(data[key]) #this way I get only the list of categories without the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look into the parameters of the vectorizer, stop_words I could use!\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_categories)\n",
    "print(type(train_data_features))\n",
    "print(train_data_features.shape)\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(type(train_data_features))\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for i,j in zip(vocab, dist):\n",
    "    if j>500:\n",
    "        print(j,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(dist,bins=np.logspace(1.4, 3.2, 50), range=(0,4000), density=False)\n",
    "hist[7]    #around bin#7 I have hit 500 less common categories, that corresponds to about an occurrency of 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shouldn't be an easier way to plot an histogram?\n",
    "%matplotlib inline \n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.ylabel('CategoryLinks')\n",
    "plt.xlabel('Occurrencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove useless words?? I can I judge that? The algorithm will tell what is really useless, right?!\n",
    "###This is the point of Machine learning... I don't have to do the work, but the MACHINE!\n",
    "\n",
    "#then I would do the same thing, but parsing the words from the categories separately\n",
    "#Then the same thing but including all the links in the article from the other table\n",
    "#and other links: formatting?\n",
    "\n",
    "#one json for categorylinks, with one key per article and a table of elements (links)\n",
    "#I should organize everything in functions and push the code to github\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", data_dic['307'][2] ) #this is a way to get only letters and remove simbols and underscore\n",
    "print(letters_only)\n",
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words\n",
    "print(words[:20])\n",
    "merge=\" \".join(words)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######___PYTHON__NOTES___######\n",
    "\n",
    "#this is how you concatenate np arrays\n",
    "\n",
    "try1=np.array([])\n",
    "a1=np.array([1,2,3])\n",
    "a2=np.array([4,5,6])\n",
    "try1=np.concatenate((a1,a2),axis=0)\n",
    "try1\n",
    "\n",
    "#to create a dictionary\n",
    "\n",
    "data_dic={}\n",
    "current_id=\"0\"                                     #CAVEAT: make sure the first page_id is not \"0\"\n",
    "for item in range(len(data)):                             #Iterate through the all the categorylink table\n",
    "    page_id=data[item][0].decode(\"utf-8\")          #estract the page_id from the first column\n",
    "    if page_id == current_id :                     #page_id not new\n",
    "        table.append(data[item][2].decode(\"utf-8\"))\n",
    "        data_dic[page_id]=table\n",
    "    else:                                          #new page_id\n",
    "        table=[]\n",
    "        table.append(data[item][2].decode(\"utf-8\"))\n",
    "        data_dic[page_id]=table                    #add value to the table and the dictionary\n",
    "        current_id=page_id\n",
    "\n",
    "\n",
    "#json_data=json.dumps(data_dic) #this dumps the dictionary into a json-string that can be .loads back into a json-object\n",
    "#retrieve with json.loads()\n",
    "#print(json_data)\n",
    "\n",
    "#if I want to take elements of a list and merge them in a string\n",
    "for key in data_dic:\n",
    "    data_dic[key]=\" \".join(data_dic[key])\n",
    "data_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep the really specific topics that occur above some threshold of pages (5%? 10%?), then split the topics into words, filter stop words, and the bag of words to get things that are common enough to model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I should try and run my own random forest, but first\n",
    "#I should get a sample of non edit-protected articles (what if they have other kinds of protection?)\n",
    "#then mix them with my dataset, make sure they are labelled, split in training and validation sets \n",
    "#train a random forest algorithm\n",
    "#To do that I will have to modify the query and I might want to branch off this notebook, so first I should push it on github \n",
    "#And this is a good time to re-practice that"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
