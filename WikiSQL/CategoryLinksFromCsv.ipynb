{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First connect through SSH tunnel to tools-login.wmflabs.org server.\n",
    "\n",
    "Replace the username and password (below) with those you find in the replica.my.cnf file in the host home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql                                               # to get, do pip install PyMySQL\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #from the NLP tutorial\n",
    "import re                                                    #from NLP tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This connects to the Wiki database\n",
    "\n",
    "username = 'u15068'                # get from replica.my.cnf\n",
    "password = 'G5V2xrWbdFTSkyoU'      # get from replica.my.cnf\n",
    "db=pymysql.connect(host='localhost',port=4711,user=username,passwd=password,db='enwiki_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10026, 10) 10026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300      998108\n",
       "301    24561971\n",
       "302     6314225\n",
       "303    49450359\n",
       "304    11432297\n",
       "Name: page_id, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the epa id numbers from the dataset on github\n",
    "# for edit protected articles: \"070916_edit_protected_articles.csv\"\n",
    "\n",
    "nepa_articles = pd.read_csv(\"../Sampling/071816_batch1_sampled_ids.csv\")   \n",
    "print(nepa_articles.shape, len(nepa_articles))\n",
    "nepa_ids=nepa_articles[\"page_id\"]\n",
    "nepa_ids[300:305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION to Quary the wiki database for caterogylinks a page_id at a time\n",
    "# To use it you need to be connected to a database through PyMySQL\n",
    "\n",
    "# The result is a dictionary in the format {\"page_id\" : [\"link1\", \"link2\", ...]}\n",
    "\n",
    "def GetDictWikiLinks(database, art_ids):\n",
    "    data={}\n",
    "    for i in range(len(art_ids)):       #to get them all len(epa_articles)\n",
    "        cur = database.cursor()               #this is how you use pymusql apparently\n",
    "        if i %1000 ==0:                 #progress print statement\n",
    "            print(\"Quarry article\", art_ids[i])\n",
    "    \n",
    "        #########################_____SQL_____##############################\n",
    "        # This is the SQL command to quarry the Wikipedia database         #\n",
    "        # The syntax of a quarry is:                                       #\n",
    "        # SELECT [columns] FROM [table] WHERE [conditions] LIMIT [optional]#\n",
    "\n",
    "# QUARRY FOR CATEGORYLINKS\n",
    "#        quarry=\"\"\"\n",
    "#        SELECT cl_from, cl_to\n",
    "#        FROM categorylinks\n",
    "#        WHERE cl_from = %s\"\"\"\n",
    "\n",
    "# QUARRY FOR PAGETITLES\n",
    "#        quarry=\"\"\"\n",
    "#        SELECT page_id, page_title\n",
    "#        FROM page\n",
    "\n",
    "        quarry=\"\"\"\n",
    "        SELECT pl_from, pl_title\n",
    "        FROM pagelinks\n",
    "        WHERE pl_from = %s\"\"\"\n",
    "        cur.execute(quarry, np.asscalar(art_ids[i]))\n",
    "        links = np.array(cur.fetchall())\n",
    "    \n",
    "        #Some quarries might be empty \n",
    "        if links.size is 0:\n",
    "            print(\"Empty categorylink %s\", art_ids[i])\n",
    "        else:   \n",
    "            key=links[0,0].decode(\"utf-8\")\n",
    "            data[key]=[links[l,1].decode(\"utf-8\") for l in range(len(links[:,1]))]\n",
    "    \n",
    "        #might take about 10 minuts (10k quarries, or 30k links)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarry article 7960121\n",
      "Quarry article 940510\n",
      "Quarry article 43629054\n",
      "Quarry article 30602946\n",
      "Quarry article 8663808\n",
      "Quarry article 37880271\n",
      "Quarry article 23566997\n",
      "Quarry article 44340180\n",
      "Quarry article 15397737\n",
      "Quarry article 20617631\n",
      "Quarry article 3064743\n"
     ]
    }
   ],
   "source": [
    "# Get the dictionary from Wikipedia database\n",
    "\n",
    "data=GetDictWikiLinks(db, nepa_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10026"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nepa_links = pd.DataFrame.from_dict(data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4055</th>\n",
       "      <th>4056</th>\n",
       "      <th>4057</th>\n",
       "      <th>4058</th>\n",
       "      <th>4059</th>\n",
       "      <th>4060</th>\n",
       "      <th>4061</th>\n",
       "      <th>4062</th>\n",
       "      <th>4063</th>\n",
       "      <th>4064</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3893800</th>\n",
       "      <td>Abattoir_(band)</td>\n",
       "      <td>Aggressive_Rock_Produktionen</td>\n",
       "      <td>And_One</td>\n",
       "      <td>Angry_Samoans</td>\n",
       "      <td>BMG_Rights_Management</td>\n",
       "      <td>Bathory_(band)</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Billboard_(magazine)</td>\n",
       "      <td>Bitch_(band)</td>\n",
       "      <td>Black_Flag_(band)</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468393</th>\n",
       "      <td>Alplaus_Maritime_Academy</td>\n",
       "      <td>Barn</td>\n",
       "      <td>Contributing_property</td>\n",
       "      <td>Cultural_artifact</td>\n",
       "      <td>Dutch_barn</td>\n",
       "      <td>Edmund_Andros</td>\n",
       "      <td>Erie_Canal</td>\n",
       "      <td>Geographic_coordinate_system</td>\n",
       "      <td>Historic_districts_in_the_United_States</td>\n",
       "      <td>History_of_the_National_Register_of_Historic_P...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71996</th>\n",
       "      <td>'Ndrangheta</td>\n",
       "      <td>2013_mass_surveillance_disclosures</td>\n",
       "      <td>ARM_architecture</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>AT&amp;T_Mobility</td>\n",
       "      <td>AT&amp;T_Tilt</td>\n",
       "      <td>Advanced_Encryption_Standard</td>\n",
       "      <td>Android_(operating_system)</td>\n",
       "      <td>Associated_Press</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46416697</th>\n",
       "      <td>Cell_fusion</td>\n",
       "      <td>Cold_sores</td>\n",
       "      <td>Digital_object_identifier</td>\n",
       "      <td>Doctorate_of_Science</td>\n",
       "      <td>Gender_equality</td>\n",
       "      <td>Genital_herpes</td>\n",
       "      <td>Herpes_Simplex_Virus</td>\n",
       "      <td>Herpes_simplex_virus</td>\n",
       "      <td>Herpes_simplex_virus_1</td>\n",
       "      <td>Herpes_simplex_virus_2</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028353</th>\n",
       "      <td>Alex_Shane</td>\n",
       "      <td>All-England_Championship</td>\n",
       "      <td>All_Star_Wrestling</td>\n",
       "      <td>Andrew_Simmons</td>\n",
       "      <td>Atholl_Oakeley</td>\n",
       "      <td>Big_Brother_2007_(UK)</td>\n",
       "      <td>British_National_Championship</td>\n",
       "      <td>Broxbourne</td>\n",
       "      <td>Chairman_(official)</td>\n",
       "      <td>Channel_4</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4065 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0                                   1     \\\n",
       "3893800            Abattoir_(band)        Aggressive_Rock_Produktionen   \n",
       "4468393   Alplaus_Maritime_Academy                                Barn   \n",
       "71996                  'Ndrangheta  2013_mass_surveillance_disclosures   \n",
       "46416697               Cell_fusion                          Cold_sores   \n",
       "4028353                 Alex_Shane            All-England_Championship   \n",
       "\n",
       "                               2                     3     \\\n",
       "3893800                     And_One         Angry_Samoans   \n",
       "4468393       Contributing_property     Cultural_artifact   \n",
       "71996              ARM_architecture                  AT&T   \n",
       "46416697  Digital_object_identifier  Doctorate_of_Science   \n",
       "4028353          All_Star_Wrestling        Andrew_Simmons   \n",
       "\n",
       "                           4                      5     \\\n",
       "3893800   BMG_Rights_Management         Bathory_(band)   \n",
       "4468393              Dutch_barn          Edmund_Andros   \n",
       "71996             AT&T_Mobility              AT&T_Tilt   \n",
       "46416697        Gender_equality         Genital_herpes   \n",
       "4028353          Atholl_Oakeley  Big_Brother_2007_(UK)   \n",
       "\n",
       "                                   6                             7     \\\n",
       "3893800                          Berlin          Billboard_(magazine)   \n",
       "4468393                      Erie_Canal  Geographic_coordinate_system   \n",
       "71996      Advanced_Encryption_Standard    Android_(operating_system)   \n",
       "46416697           Herpes_Simplex_Virus          Herpes_simplex_virus   \n",
       "4028353   British_National_Championship                    Broxbourne   \n",
       "\n",
       "                                             8     \\\n",
       "3893800                              Bitch_(band)   \n",
       "4468393   Historic_districts_in_the_United_States   \n",
       "71996                            Associated_Press   \n",
       "46416697                   Herpes_simplex_virus_1   \n",
       "4028353                       Chairman_(official)   \n",
       "\n",
       "                                                       9     ...   4055  4056  \\\n",
       "3893800                                   Black_Flag_(band)  ...   None  None   \n",
       "4468393   History_of_the_National_Register_of_Historic_P...  ...   None  None   \n",
       "71996                                               Atlanta  ...   None  None   \n",
       "46416697                             Herpes_simplex_virus_2  ...   None  None   \n",
       "4028353                                           Channel_4  ...   None  None   \n",
       "\n",
       "          4057  4058  4059  4060  4061  4062  4063  4064  \n",
       "3893800   None  None  None  None  None  None  None  None  \n",
       "4468393   None  None  None  None  None  None  None  None  \n",
       "71996     None  None  None  None  None  None  None  None  \n",
       "46416697  None  None  None  None  None  None  None  None  \n",
       "4028353   None  None  None  None  None  None  None  None  \n",
       "\n",
       "[5 rows x 4065 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nepa_links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_nepa_links.to_csv('nepa_links_titles_batch1_encoded2.csv', encoding='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nepa_links.to_csv('nepa_links_titles_batch1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write and save data as a JSON to file\n",
    "\n",
    "with open('pl_nepa_2.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        # Examples of querries!\n",
    "        \n",
    "        # to get the category links\n",
    "        # SELECT cl_from, cl_to\n",
    "        # FROM categorylinks\n",
    "        # WHERE cl_from = %s\n",
    "        \n",
    "        # To get the page-links\n",
    "        # SELECT pl_from, pl_title\n",
    "        # FROM pagelinks\n",
    "        # WHERE pl_from = %s\n",
    "        # AND pl_namespace = 0\n",
    "        \n",
    "        # to get stuff from different tables:\n",
    "        # SELECT page_id, cl_to\n",
    "        # FROM page inner join categorylinks on(page.page_id = categorylinks.cl_from)\n",
    "        # WHERE page_id = %s\n",
    "        # actually that could have been simplyfied, but it nicely shows how to merge tables with \"inner join\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################\n",
    "\n",
    "Now I need to transform the categorylinks file by splitting the links in individual words\n",
    "\n",
    "I will define a function called GetWordsList()\n",
    "\n",
    "    cl_epa.txt \n",
    "    cl_words_epa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"cl_epa.txt\", \"r\") as f:\n",
    "    data=json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"25\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is to get the same files but with individual words separated \n",
    "\n",
    "import re\n",
    "\n",
    "def GetWordList(data_dict):\n",
    "    #data_dict is a dictionary {keys}:[\"str1\", \"str2\",..., \"strN\"]\n",
    "    #this functions yields [\"str1\", \"str2\", ... \"strM\"], where M is the sum of N over all Keys\n",
    "    list_categories = []\n",
    "    cl_index=[]\n",
    "    for key in data_dict: \n",
    "        cl_index.append(key)\n",
    "        links=[]\n",
    "        for n in range(len(data_dict[key])):\n",
    "        \n",
    "            # Remove the following regular expressions\n",
    "            \n",
    "            re0=\"protected_page\"\n",
    "            #re1=\"with_accessdate\"\n",
    "            #re2=\".rticle\"\n",
    "            re4=\".ikipedia\"\n",
    "            #re4=\"to_be_expanded\"\n",
    "            #re5=\"with_unsourced_statements\"\n",
    "            #re6=\"needing_additional_references\"\n",
    "            #re7=\"lacking_sources\"\n",
    "            #re8=\"containing_potentially_dated_statements\"\n",
    "            #re9=\"with_dead_external_links\"\n",
    "            re1=\"Certification_Table_Entry_usages_for\"\n",
    "            re2=\"language_sources\"\n",
    "            re3=\"Articles_containing\"\n",
    "            \n",
    "            if not any(re.search(regex, data_dict[key][n]) for regex in [re0, re1, re2, re3, re4]):\n",
    "                words=data_dict[key][n].replace(\"_\", \" \")\n",
    "                links.append(words)\n",
    "        links=\" \".join(links)\n",
    "        list_categories.append(links)\n",
    "                \n",
    "    return list_categories, cl_index      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_categories, epa_id = GetWordList(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(clean_categories)\n",
    "clean_categories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[epa_id[1]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_dict={}\n",
    "\n",
    "for i in range(len(clean_categories)):\n",
    "    words_dict[epa_id[i]]=clean_categories[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_dict[\"19283178\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('cl_words_epa.txt', 'w') as outfile:\n",
    "    json.dump(words_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"cl_words_epa.txt\", \"r\") as f:\n",
    "    clw=json.load(f)\n",
    "len(clw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clw[\"13259888\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(clean_categories)):\n",
    "    \n",
    "link=clean_categories[0].split()\n",
    "words=link[1].split(\"_\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#then I would do the same thing, but parsing the words from the categories separately\n",
    "#Then the same thing but including all the links in the article from the other table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep the really specific topics that occur above some threshold of pages (5%? 10%?), then split the topics into words, filter stop words, and the bag of words to get things that are common enough to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I should try and run my own random forest, but first\n",
    "#I should get a sample of non edit-protected articles (what if they have other kinds of protection?)\n",
    "#then mix them with my dataset, make sure they are labelled, split in training and validation sets \n",
    "#train a random forest algorithm\n",
    "#To do that I will have to modify the query and I might want to branch off this notebook, so first I should push it on github \n",
    "#And this is a good time to re-practice that"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
