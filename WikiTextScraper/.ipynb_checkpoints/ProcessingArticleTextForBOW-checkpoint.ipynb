{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "epa_ft_url = \"https://storage.googleapis.com/atriskwiki/epa_fulltext.txt\"\n",
    "nepa_ft_url = \"https://storage.googleapis.com/atriskwiki/nepa_fulltext.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\",25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def StemFTfromCSV(csv_fn):\n",
    "    now = datetime.datetime.now().time()\n",
    "    print(\"[{:02d}:{:02d}:{:02d}] Opening file: {}\".format(now.hour,now.minute,now.second,csv_fn))\n",
    "    processedTexts = dict()\n",
    "    with open(csv_fn,\"r\") as f:\n",
    "        for ix, line in enumerate(f):\n",
    "            if (ix + 1) % 1000 == 0:\n",
    "                now = datetime.datetime.now().time()\n",
    "                print(\"[{:02d}:{:02d}:{:02d}] Processing record {}\".format(now.hour,now.minute,now.second,ix+1))\n",
    "            try:\n",
    "                ft_json = json.loads(line)\n",
    "                article_id = list(ft_json.keys())[0]\n",
    "                processedTexts[article_id] = GetAndCleanArticle(ft_json[article_id],fulltext=True)\n",
    "            except Exception as e:\n",
    "                now = datetime.datetime.now().time()\n",
    "                print(\"[{:02d}:{:02d}:{:02d}] {}:\\n{}\".format(now.hour,now.minute,now.second,e,line[:140]))\n",
    "            \n",
    "    df_ft = pd.DataFrame.from_dict(processedTexts, orient=\"index\")\n",
    "    df_ft.index.name = 'article_id'\n",
    "    df_ft.columns = ['ft_stems']\n",
    "            \n",
    "    return df_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetFTLength(fulltext):\n",
    "    fulltext = re.sub(r\"\\n\",\" \",fulltext)\n",
    "    fulltext = re.sub(r\"[^A-Za-z ]\",\"\",fulltext)\n",
    "    return len(fulltext.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:08:52] Opening file: ArticleFulltexts/epa_fulltext.txt\n",
      "[10:09:37] Processing record 1000\n",
      "[10:10:05] Processing record 2000\n",
      "[10:10:25] Processing record 3000\n",
      "[10:10:39] Processing record 4000\n",
      "[10:10:55] Processing record 5000\n",
      "[10:11:09] Processing record 6000\n",
      "[10:11:20] Processing record 7000\n"
     ]
    }
   ],
   "source": [
    "stemmed_fts = StemFTfromCSV(\"ArticleFulltexts/epa_fulltext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:12:06] Opening file: ArticleFulltexts/071816_fulltext_batch1.txt\n",
      "[10:12:22] Processing record 1000\n",
      "[10:12:35] Processing record 2000\n",
      "[10:12:44] Processing record 3000\n",
      "[10:12:52] Processing record 4000\n",
      "[10:12:59] Processing record 5000\n",
      "[10:13:04] Processing record 6000\n",
      "[10:13:10] Processing record 7000\n",
      "[10:13:17] Processing record 8000\n",
      "[10:13:22] Processing record 9000\n",
      "[10:13:26] Opening file: ArticleFulltexts/071816_fulltext_batch2.txt\n",
      "[10:13:42] Processing record 1000\n",
      "[10:13:53] Processing record 2000\n",
      "[10:14:02] Processing record 3000\n",
      "[10:14:09] Processing record 4000\n",
      "[10:14:14] Processing record 5000\n",
      "[10:14:21] Processing record 6000\n",
      "[10:14:26] Processing record 7000\n",
      "[10:14:32] Processing record 8000\n",
      "[10:14:34] expected string or bytes-like object:\n",
      "{\"51074306\": null}\n",
      "\n",
      "[10:14:34] Opening file: ArticleFulltexts/071816_fulltext_batch3.txt\n",
      "[10:14:50] Processing record 1000\n",
      "[10:15:01] Processing record 2000\n",
      "[10:15:10] Processing record 3000\n",
      "[10:15:19] Processing record 4000\n",
      "[10:15:26] Processing record 5000\n",
      "[10:15:31] Processing record 6000\n",
      "[10:15:38] Processing record 7000\n",
      "[10:15:44] Processing record 8000\n",
      "[10:15:49] Processing record 9000\n",
      "[10:15:53] Opening file: ArticleFulltexts/071816_fulltext_batch4.txt\n",
      "[10:16:09] Processing record 1000\n",
      "[10:16:21] Processing record 2000\n",
      "[10:16:30] Processing record 3000\n",
      "[10:16:38] Processing record 4000\n",
      "[10:16:46] Processing record 5000\n",
      "[10:16:52] Processing record 6000\n",
      "[10:16:58] Processing record 7000\n",
      "[10:17:04] Processing record 8000\n",
      "[10:17:09] Processing record 9000\n",
      "[10:17:13] Opening file: ArticleFulltexts/071816_fulltext_batch5.txt\n",
      "[10:17:30] Processing record 1000\n",
      "[10:17:42] Processing record 2000\n",
      "[10:17:52] Processing record 3000\n",
      "[10:18:01] Processing record 4000\n",
      "[10:18:08] Processing record 5000\n",
      "[10:18:14] Processing record 6000\n",
      "[10:18:20] Processing record 7000\n",
      "[10:18:26] Processing record 8000\n",
      "[10:18:31] Processing record 9000\n",
      "[10:18:36] Opening file: ArticleFulltexts/071816_fulltext_batch6.txt\n",
      "[10:18:54] Processing record 1000\n",
      "[10:19:07] Processing record 2000\n",
      "[10:19:16] Processing record 3000\n",
      "[10:19:23] Processing record 4000\n",
      "[10:19:30] Processing record 5000\n",
      "[10:19:36] Processing record 6000\n",
      "[10:19:42] Processing record 7000\n",
      "[10:19:47] Processing record 8000\n",
      "[10:19:52] Processing record 9000\n",
      "[10:19:54] expected string or bytes-like object:\n",
      "{\"44972286\": null}\n",
      "\n",
      "[10:19:56] Opening file: ArticleFulltexts/071816_fulltext_batch7.txt\n",
      "[10:20:13] Processing record 1000\n",
      "[10:20:25] Processing record 2000\n",
      "[10:20:35] Processing record 3000\n",
      "[10:20:43] Processing record 4000\n",
      "[10:20:50] Processing record 5000\n",
      "[10:20:53] expected string or bytes-like object:\n",
      "{\"18076569\": null}\n",
      "\n",
      "[10:20:56] Processing record 6000\n",
      "[10:21:01] Processing record 7000\n",
      "[10:21:02] expected string or bytes-like object:\n",
      "{\"29310534\": null}\n",
      "\n",
      "[10:21:07] Processing record 8000\n",
      "[10:21:13] Processing record 9000\n",
      "[10:21:17] Opening file: ArticleFulltexts/071816_fulltext_batch8.txt\n",
      "[10:21:33] Processing record 1000\n",
      "[10:21:44] Processing record 2000\n",
      "[10:21:52] Processing record 3000\n",
      "[10:21:59] Processing record 4000\n",
      "[10:22:05] Processing record 5000\n",
      "[10:22:11] Processing record 6000\n",
      "[10:22:16] Processing record 7000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"ArticleFulltexts/\"\n",
    "for filename in os.listdir(path):\n",
    "    if filename == \"epa_fulltext.txt\":\n",
    "        pass\n",
    "    else:\n",
    "        stemmed = StemFTfromCSV(path + filename)\n",
    "        stemmed_fts = pd.concat([stemmed_fts, stemmed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ScrapeAndStemIntros(fulltextsURL):\n",
    "    \"\"\"Retrieves JSON-formatted article fulltexts from the specified URL and returns a pandas.DataFrame\n",
    "    containing stemmed introduction strings for each article, indexed by article_id\n",
    "    \"\"\"\n",
    "    print(\"Getting remote file...\")\n",
    "    fulltexts = requests.get(fulltextsURL).text.split(\"\\n\")\n",
    "    print(\"File retrieved and split!\")\n",
    "    processedTexts = dict()\n",
    "    \n",
    "    print(\"Stemming records...\")\n",
    "    for (ix, ft) in enumerate(fulltexts):\n",
    "        if (ix + 1) % 500 == 0:\n",
    "            print(\"Stemming record {}.\".format(ix + 1))\n",
    "        try:\n",
    "            ft_json = json.loads(ft)\n",
    "            article_id = list(ft_json.keys())[0]\n",
    "            processedTexts[article_id] = GetAndCleanArticle(ft_json[article_id])\n",
    "        except:\n",
    "            print(\"RECORD FAILED: {} {}\".format(article_id,ft))\n",
    "    \n",
    "    df_intro = pd.DataFrame.from_dict(processedTexts, orient=\"index\")\n",
    "    df_intro.index.name = 'article_id'\n",
    "    df_intro.columns = ['intro_stems']\n",
    "    \n",
    "    return df_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetAndCleanArticle(text, fulltext=False):\n",
    "    \"\"\"Expects a string containing a wikipedia article formatted with explaintext and exsectionformat=wiki\n",
    "    Returns a list containing all Porter stemmed non-stop words from the introductory section of the article\n",
    "    (can be set to process the entire article with fulltext=True)\"\"\"\n",
    "    if not fulltext:\n",
    "        intro = re.compile(\"==.*?==\").split(text)[0]\n",
    "    else:\n",
    "        intro = text\n",
    "    intro = re.sub(r\"\\n\",\" \",intro)\n",
    "    intro = re.sub(r\"[^A-Za-z ]\",\"\",intro)\n",
    "    intro = intro.lower()\n",
    "    intro_words = intro.split()\n",
    "    intro_words = [stemmer.stem(w) for w in intro_words if w not in stops]\n",
    "    return \" \".join(intro_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def VectorizeMostCommonFeatures(dataframe, num_features):\n",
    "    \"\"\"Identifies the num_features most common features from a dataframe generated by ScrapeAndStemIntros\n",
    "    containing space-delimited stemmed strings.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features = num_features)\n",
    "    vec_words = vectorizer.fit_transform(dataframe[\"ft_stems\"])\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    count_vocab = vec_words.toarray()\n",
    "    count_df = pd.DataFrame(count_vocab, columns=vocab, index=dataframe.index)\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_vec_count = VectorizeMostCommonFeatures(stemmed_fts, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa              1774\n",
       "aaa             1228\n",
       "aaron           2500\n",
       "ab              2162\n",
       "abandon         8505\n",
       "abba            1101\n",
       "abbey           3470\n",
       "abbot           1050\n",
       "abbott          1261\n",
       "abbrevi         2059\n",
       "abc             5367\n",
       "abd              719\n",
       "               ...  \n",
       "zionism          605\n",
       "zionist          963\n",
       "zip              770\n",
       "zodiac           621\n",
       "zombi           1449\n",
       "zone           11087\n",
       "zoo             2369\n",
       "zoolog           653\n",
       "zoroastrian      650\n",
       "zrich            636\n",
       "zu               603\n",
       "zur              755\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get vectors for edit protected and sample articles separately\n",
    "#get counts for union of those sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_vec_count = VectorizeMostCommonFeatures(epa_stem_intros, 5000)\n",
    "nepa_vec_count = VectorizeMostCommonFeatures(nepa_stem_intros, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_entries_with_words = epa_vec_count[epa_vec_count.sum(axis=1) > 0].index\n",
    "epa_vecs_with_words = epa_vec_count[epa_vec_count.sum(axis=1) > 0].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7526"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = np.sum(epa_vec_count, axis=0)\n",
    "epa_vec_count.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ProcessWordOccurrences(vec_count_df):\n",
    "    '''Given a dataframe (rows: articles, cols: word counts) containg the output of VectorizeMostCommonFeatures, output a dataframe\n",
    "    containing the number of occurrences of each word across the dataset, the number of articles with at least\n",
    "    one occurrence of the word, and the fraction of articles with at least once occurrence.'''\n",
    "    vocab = vec_count_df.columns.values\n",
    "    count_by_word = np.sum(vec_count_df, axis=0)\n",
    "    \n",
    "    present = vec_count_df > np.zeros(vec_count_df.shape)\n",
    "    present_by_word = np.sum(present, axis=0)\n",
    "    present_by_word_frac = present_by_word / vec_count_df.shape[0]\n",
    "    \n",
    "    df = pd.concat([count_by_word,present_by_word,present_by_word_frac],axis=1)\n",
    "    df.columns = ['occurences','articles','frac_of_articles']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_occurrences = ProcessWordOccurrences(epa_vec_count)\n",
    "nepa_occurrences = ProcessWordOccurrences(nepa_vec_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occ = pd.merge(epa_occurrences, nepa_occurrences, left_index=True, right_index=True, suffixes=('_epa','_nepa'), indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occ.to_csv(path_or_buf=\"BOW_5000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "occ_all = pd.merge(epa_occurences, nepa_occurrences, how=\"outer\", left_index=True, right_index=True, suffixes=('_epa','_nepa'), indicator=True)\n",
    "#occ_all.to_csv(path_or_buf=\"BOW_5000_ALL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UnitVector(vector):\n",
    "    return vector/np.linalg.norm(vector)\n",
    "\n",
    "def AngleBetween(v1, v2):\n",
    "    v1_u = UnitVector(v1)\n",
    "    v2_u = UnitVector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_norms = np.linalg.norm(epa_vecs_with_words, axis=1) #calculates norms of row vectors\n",
    "epa_vecs_normed = epa_vecs_with_words / epa_norms[:,None] #unitizes row vectors\n",
    "distances = 180 * np.arccos(np.clip(np.dot(epa_vecs_normed,epa_vecs_normed.T),-1.0, 1.0)) / np.pi #calculates the angle\n",
    "#in degrees between each pair of articles\n",
    "epa_article_distances = pd.DataFrame(distances, index=epa_entries_with_words, columns=epa_entries_with_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2872345', '598524', '35901300', '6110688', '22705211', '4996831',\n",
       "       '15296435', '8933089', '23324667', '18940588',\n",
       "       ...\n",
       "       '36599245', '12120000', '83019', '81083', '21552009', '47125',\n",
       "       '10483209', '33947707', '33591033', '405134'],\n",
       "      dtype='object', name='article_id', length=7489)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa_entries_with_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "23324667     0.000002\n",
       "728093      64.872575\n",
       "22785154    67.860228\n",
       "241547      67.921119\n",
       "52271       68.460381\n",
       "2529569     69.138747\n",
       "161041      69.292024\n",
       "4689709     69.506649\n",
       "72671       69.720620\n",
       "579570      69.875869\n",
       "13076       70.075230\n",
       "993655      70.356555\n",
       "1102585     70.560807\n",
       "67119       70.587929\n",
       "19279158    70.603681\n",
       "415700      70.650424\n",
       "451733      70.919237\n",
       "24117       70.935768\n",
       "15674472    70.950998\n",
       "5043544     70.962876\n",
       "419342      71.280068\n",
       "376619      71.508761\n",
       "169798      71.606063\n",
       "20652023    71.636490\n",
       "149333      71.678676\n",
       "69926       71.956406\n",
       "Name: 23324667, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetMostSimilar(article_id, numrecords=25):\n",
    "    '''Given an article ID, return a list of the most similar (i.e., lowest angle) articles in the dataset'''\n",
    "    return epa_article_distances[str(article_id)].sort_values()[:numrecords+1]\n",
    "\n",
    "GetMostSimilar(23324667)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzHAyMDMznAzMzAwnAzMzA/4/kRs2g2I33tIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131480400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#distance scaling function, engineered to increase slowly up to 70 and rapidly thereafter\n",
    "x_plot = np.arange(0,90,0.5)\n",
    "y_plot = (100 + x_plot) / (85 - x_plot) \n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.17647062,  400.        ,  398.02583958, ...,  355.26924076,\n",
       "         105.74430349,  400.        ],\n",
       "       [ 400.        ,    1.17647059,  400.        , ...,  400.        ,\n",
       "         400.        ,  400.        ],\n",
       "       [ 398.02583958,  400.        ,    1.17647063, ...,  400.        ,\n",
       "         400.        ,  400.        ],\n",
       "       ..., \n",
       "       [ 355.26924076,  400.        ,  400.        , ...,    1.17647059,\n",
       "         400.        ,  400.        ],\n",
       "       [ 105.74430349,  400.        ,  400.        , ...,  400.        ,\n",
       "           1.17647059,  400.        ],\n",
       "       [ 400.        ,  400.        ,  400.        , ...,  400.        ,\n",
       "         400.        ,    1.17647059]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_distances = (100 + distances) / (85 - distances)\n",
    "scaled_distances[scaled_distances <= 0] = 400 #sets negative values (i.e., angles above 85) to an arbitrarily large distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "topic_DBSCAN = DBSCAN(eps=12, min_samples=10, metric='precomputed')\n",
    "topic_labels = topic_DBSCAN.fit_predict(scaled_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CharacterizeClassPop(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), determine how many\n",
    "    non-noise clusters (i.e., some number besides -1) were assigned, what fraction of articles were assigned to a\n",
    "    non-noise cluster, and the average, median, and max size of the population of clusters. Returns a dict.'''\n",
    "    classCounts = np.array(np.unique(topic_labels, return_counts=True)).T #each row contains the class label and the number of occurrences\n",
    "    assignedClasses = classCounts[1:]\n",
    "    numClust = assignedClasses.shape[0]\n",
    "    fracInClust = np.sum(assignedClasses[:,1]) / np.sum(classCounts[:,1])\n",
    "    avgClustSize = np.sum(assignedClasses[:,1]) / numClust\n",
    "    medianClustSize = np.median(assignedClasses[:,1])\n",
    "    maxClustSize = np.max(assignedClasses[:,1])\n",
    "    \n",
    "    return {'numClusters':numClust,\n",
    "           'coverage':fracInClust,\n",
    "           'avgClusterSize':avgClustSize,\n",
    "           'medianClusterSize':medianClustSize,\n",
    "           'maxClusterSize':maxClustSize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def InfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), select <= 10 representative\n",
    "    articles from each class, and the 10 most frequently present words from that cluster. Returns a nested dict.'''\n",
    "    id_topic = pd.DataFrame(topic_labels, index=epa_entries_with_words, columns=['DB_class'])\n",
    "    vec_words_by_class = pd.merge(epa_vec_count, id_topic,\n",
    "                                 how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    id_topic.index = id_topic.index.map(int)\n",
    "    titles_by_class = pd.merge(epa_titles, id_topic, \n",
    "                               how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    classInfo = dict()\n",
    "    for k in np.unique(topic_labels)[1:].tolist():\n",
    "        classInfo[k] = dict()\n",
    "        k_words = vec_words_by_class[vec_words_by_class['DB_class'] == k]\n",
    "        classMem = k_words.shape[0]\n",
    "        classInfo[k]['class_members'] = classMem\n",
    "        present = k_words > np.zeros(k_words.shape)\n",
    "        present_by_word = np.sum(present, axis=0)\n",
    "        present_by_word_frac = present_by_word / k_words.shape[0]\n",
    "        classInfo[k]['top_words'] = present_by_word_frac.sort_values(ascending=False)[:10].to_dict()\n",
    "        \n",
    "        sampSize = min(classMem, 10)\n",
    "        k_titles = titles_by_class[titles_by_class['DB_class'] == k].sample(n=sampSize)\n",
    "        classInfo[k]['rep_titles'] = k_titles['page_title'].tolist()\n",
    "    return classInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "CLUSTER 0: 5275 MEMBERS\n",
      "\n",
      "Representative articles:\n",
      "Victoria_Justice\n",
      "Michael_Oher\n",
      "Suicide_Squad_(film)\n",
      "Albert_Pyun\n",
      "Machete_(film)\n",
      "Landon_Hedges\n",
      "List_of_Hail_Mary_passes_in_American_football\n",
      "Puerto_Ricans\n",
      "Mahatma_Gandhi\n",
      "List_of_Teletubbies_episodes_and_videos\n",
      "\n",
      "Most common terms:\n",
      "also: 0.383\n",
      "first: 0.317\n",
      "american: 0.316\n",
      "one: 0.302\n",
      "born: 0.300\n",
      "includ: 0.296\n",
      "state: 0.272\n",
      "known: 0.269\n",
      "year: 0.241\n",
      "unit: 0.240\n",
      "========================\n",
      "========================\n",
      "CLUSTER 1: 14 MEMBERS\n",
      "\n",
      "Representative articles:\n",
      "Water\n",
      "Evaporation\n",
      "Cactus\n",
      "Rain\n",
      "Renewable_energy\n",
      "Water_fluoridation\n",
      "Ocean\n",
      "Coral_reef\n",
      "Water_fluoridation_controversy\n",
      "Photosynthesis\n",
      "\n",
      "Most common terms:\n",
      "DB_class: 1.000\n",
      "water: 0.929\n",
      "use: 0.786\n",
      "also: 0.714\n",
      "form: 0.643\n",
      "earth: 0.571\n",
      "organ: 0.500\n",
      "exist: 0.500\n",
      "mani: 0.500\n",
      "global: 0.500\n",
      "========================\n",
      "========================\n",
      "CLUSTER 2: 13 MEMBERS\n",
      "\n",
      "Representative articles:\n",
      "Earth\n",
      "Google_Earth\n",
      "Moon\n",
      "Jupiter\n",
      "Uranus\n",
      "Solar_eclipse\n",
      "Mercury_(element)\n",
      "Sky\n",
      "Mercury_(planet)\n",
      "Venus\n",
      "\n",
      "Most common terms:\n",
      "DB_class: 1.000\n",
      "earth: 0.923\n",
      "sun: 0.769\n",
      "moon: 0.769\n",
      "solar: 0.769\n",
      "planet: 0.769\n",
      "system: 0.692\n",
      "orbit: 0.692\n",
      "surfac: 0.692\n",
      "natur: 0.615\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def PrintInfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), run InfoAboutEachClass() and format\n",
    "    its output for printing. Returns a formatted string.'''\n",
    "    \n",
    "    printInfo = InfoAboutEachClass(topic_labels)\n",
    "    report = \"\"\n",
    "    for key in sorted(printInfo):\n",
    "        report += \"========================\\n\"\n",
    "        report += \"CLUSTER {}: {} MEMBERS\\n\\n\".format(key, printInfo[key]['class_members'])\n",
    "        \n",
    "        report += \"Representative articles:\\n\"\n",
    "        for article in printInfo[key]['rep_titles']:\n",
    "            report += article\n",
    "            report += \"\\n\"\n",
    "        report += \"\\nMost common terms:\\n\"\n",
    "        for word in sorted(printInfo[key]['top_words'], key=printInfo[key]['top_words'].get, reverse=True):\n",
    "            report += \"{}: {:.3f}\\n\".format(word, printInfo[key]['top_words'][word])\n",
    "        report += \"========================\\n\"\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RunAndAnalyzeDBSCAN(eps, min_samples, scaled_distances):\n",
    "    '''Given a value for epsilon, the minimum number of samples required for a core point, and a matrix of\n",
    "    precomputed distances, create a DBSCAN classifier, use it to fit and predict based on the supplied distances, \n",
    "    and analyze the output using CharacterizeClassPop and PrintInfoAboutEachClass. Returns a csv formatted string\n",
    "    containing epsilon, min_samples, numClusters, coverage, avgClusterSize, medianClusterSize, and maxClusterSize'''\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clust = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    topic_labels = clust.fit_predict(scaled_distances)\n",
    "    numClusters = np.unique(topic_labels).shape[0]\n",
    "    if numClusters > 1:\n",
    "        cp = CharacterizeClassPop(topic_labels)\n",
    "        report = PrintInfoAboutEachClass(topic_labels)\n",
    "    \n",
    "        reportFN = \"candidate_clusters/{}_{}_clustReport.txt\".format(eps, min_samples)\n",
    "        with open(reportFN, \"w\") as f:\n",
    "            f.write(report)\n",
    "    \n",
    "        return \"{},{},{},{:.3f},{:.0f},{:.0f},{}\".format(eps,min_samples,cp['numClusters'],cp['coverage'],cp['avgClusterSize'],\n",
    "                                        cp['medianClusterSize'],cp['maxClusterSize'])\n",
    "    else:\n",
    "        return \"{},{},0,0,0,0,0\".format(eps,min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_eps = range(2,13)\n",
    "candidate_samples = [2,5,10,20,50,100]\n",
    "with open(\"DBSCAN_metrics.txt\",\"w\") as d:\n",
    "    d.write(\"epsilon,min_samples,num_clusters,coverage,avg_cluster_size,median_cluster_size,max_cluster_size\\n\")\n",
    "    for eps in candidate_eps:\n",
    "        for samples in candidate_samples:\n",
    "            output = RunAndAnalyzeDBSCAN(eps, samples, scaled_distances)\n",
    "            d.write(output)\n",
    "            d.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_title</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2872345</th>\n",
       "      <td>Lupe_Fiasco</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598524</th>\n",
       "      <td>Ariel_(city)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35901300</th>\n",
       "      <td>Bal_des_débutantes</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6110688</th>\n",
       "      <td>Splitting_of_the_moon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22705211</th>\n",
       "      <td>Southsound_Radio</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996831</th>\n",
       "      <td>Dean_Winchester</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15296435</th>\n",
       "      <td>Faget</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8933089</th>\n",
       "      <td>List_of_Sindhi-language_poets</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23324667</th>\n",
       "      <td>Victor_Ponta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18940588</th>\n",
       "      <td>A.C._Milan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247225</th>\n",
       "      <td>Peter_Stollery</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14687216</th>\n",
       "      <td>Justin_Beaver</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37259045</th>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21881809</th>\n",
       "      <td>Kylie_Jenner</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6704421</th>\n",
       "      <td>All_Time_Low</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39444767</th>\n",
       "      <td>Maximum_Conviction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339810</th>\n",
       "      <td>List_of_Kannada-language_television_channels</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578227</th>\n",
       "      <td>Bungle_(Rainbow)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739547</th>\n",
       "      <td>José_Mourinho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113728</th>\n",
       "      <td>Geothermal_energy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233369</th>\n",
       "      <td>Nablus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967176</th>\n",
       "      <td>Eye_of_Horus</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67173</th>\n",
       "      <td>World_Trade_Center_site</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34213720</th>\n",
       "      <td>Sirpur_(Chhattisgarh)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33963024</th>\n",
       "      <td>Washington_D.C._Area_Film_Critics_Association_...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348333</th>\n",
       "      <td>Senkaku_Islands</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72336</th>\n",
       "      <td>Barbra_Streisand</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30528002</th>\n",
       "      <td>Ed_Sheeran</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168174</th>\n",
       "      <td>Sloth</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331221</th>\n",
       "      <td>Creatine</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26889895</th>\n",
       "      <td>Colonel_Sanders</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23446581</th>\n",
       "      <td>Harrison_Barnes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166610</th>\n",
       "      <td>List_of_micronations</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29202630</th>\n",
       "      <td>Tyler,_The_Creator</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37261</th>\n",
       "      <td>Ion_Antonescu</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16243</th>\n",
       "      <td>Jawaharlal_Nehru</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27159</th>\n",
       "      <td>Sherlock_Holmes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616492</th>\n",
       "      <td>Internet_meme</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>Amphetamine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13097064</th>\n",
       "      <td>Harold_Pinter_bibliography</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25762</th>\n",
       "      <td>Russian_Revolution</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683122</th>\n",
       "      <td>Bambi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326967</th>\n",
       "      <td>Maurice_Roche,_4th_Baron_Fermoy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19019</th>\n",
       "      <td>Mr._T</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116229</th>\n",
       "      <td>Genesis_creation_narrative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326645</th>\n",
       "      <td>Chemical_change</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39837608</th>\n",
       "      <td>Artpop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458690</th>\n",
       "      <td>Allan_R._Bomhard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55523</th>\n",
       "      <td>R._Kelly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24176643</th>\n",
       "      <td>Mirror_Monster</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12992431</th>\n",
       "      <td>KidsCo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10284103</th>\n",
       "      <td>House_of_Merode</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65220</th>\n",
       "      <td>Nagorno-Karabakh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407865</th>\n",
       "      <td>Matt</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18389770</th>\n",
       "      <td>List_of_Bratz_characters</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877706</th>\n",
       "      <td>Emanuel_School</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45397</th>\n",
       "      <td>Jane_Goodall</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118299</th>\n",
       "      <td>Fulgore</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023307</th>\n",
       "      <td>TNT_(magazine)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38103</th>\n",
       "      <td>BMX</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548669</th>\n",
       "      <td>Demolition_Man_(film)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>Canadian_Radio-television_and_Telecommunicatio...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13629643</th>\n",
       "      <td>Danville_Public_Library_(Danville,_Illinois)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19196130</th>\n",
       "      <td>Nickelodeon_Australian_Kids'_Choice_Awards_2008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415036</th>\n",
       "      <td>Jeremy_Corbyn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57445</th>\n",
       "      <td>Thurgood_Marshall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13383154</th>\n",
       "      <td>Javier_Hernández</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18319111</th>\n",
       "      <td>Shilpa_Shetty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24194794</th>\n",
       "      <td>Amber_Liu_(singer)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715536</th>\n",
       "      <td>Víctor_Valdés</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32864</th>\n",
       "      <td>WikiWikiWeb</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55490</th>\n",
       "      <td>Roald_Dahl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22992135</th>\n",
       "      <td>Rapunzel_(Disney)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55212</th>\n",
       "      <td>Newton's_laws_of_motion</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8531</th>\n",
       "      <td>Dragon</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350924</th>\n",
       "      <td>Adam_Goodes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>Book_of_Mormon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298599</th>\n",
       "      <td>Blackburn_Rovers_F.C.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6924584</th>\n",
       "      <td>Predacon</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45627436</th>\n",
       "      <td>Bunny_Meyer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418326</th>\n",
       "      <td>Afzal_Guru</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29635016</th>\n",
       "      <td>Teen_Wolf_(2011_TV_series)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>Alpha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828209</th>\n",
       "      <td>Tuʻi_Tonga_Empire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351301</th>\n",
       "      <td>Rolie_Polie_Olie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19527</th>\n",
       "      <td>Mao_Zedong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186300</th>\n",
       "      <td>John_D._Rockefeller</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354872</th>\n",
       "      <td>Dick_Strawbridge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118400</th>\n",
       "      <td>Black_Orchid_(Killer_Instinct)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6341469</th>\n",
       "      <td>Pedophilia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36599245</th>\n",
       "      <td>Scott_Flanigan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12120000</th>\n",
       "      <td>27_Club</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83019</th>\n",
       "      <td>Motherfucker</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81083</th>\n",
       "      <td>Davy_Crockett</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552009</th>\n",
       "      <td>Aisha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47125</th>\n",
       "      <td>Easter_Bunny</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10483209</th>\n",
       "      <td>Skilled_Group</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33947707</th>\n",
       "      <td>Ross_Lynch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33591033</th>\n",
       "      <td>Parineeti_Chopra</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405134</th>\n",
       "      <td>Lot_in_Islam</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7489 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 page_title  0\n",
       "2872345                                         Lupe_Fiasco  0\n",
       "598524                                         Ariel_(city)  0\n",
       "35901300                                 Bal_des_débutantes -1\n",
       "6110688                               Splitting_of_the_moon  0\n",
       "22705211                                   Southsound_Radio  0\n",
       "4996831                                     Dean_Winchester  0\n",
       "15296435                                              Faget  0\n",
       "8933089                       List_of_Sindhi-language_poets  0\n",
       "23324667                                       Victor_Ponta  0\n",
       "18940588                                         A.C._Milan  0\n",
       "1247225                                      Peter_Stollery -1\n",
       "14687216                                      Justin_Beaver  0\n",
       "37259045                                          PewDiePie  0\n",
       "21881809                                       Kylie_Jenner -1\n",
       "6704421                                        All_Time_Low  0\n",
       "39444767                                 Maximum_Conviction  0\n",
       "4339810        List_of_Kannada-language_television_channels  0\n",
       "4578227                                    Bungle_(Rainbow)  0\n",
       "739547                                        José_Mourinho  0\n",
       "113728                                    Geothermal_energy -1\n",
       "233369                                               Nablus  0\n",
       "967176                                         Eye_of_Horus -1\n",
       "67173                               World_Trade_Center_site  0\n",
       "34213720                              Sirpur_(Chhattisgarh)  0\n",
       "33963024  Washington_D.C._Area_Film_Critics_Association_... -1\n",
       "348333                                      Senkaku_Islands  0\n",
       "72336                                      Barbra_Streisand  0\n",
       "30528002                                         Ed_Sheeran  0\n",
       "5168174                                               Sloth -1\n",
       "331221                                             Creatine -1\n",
       "26889895                                    Colonel_Sanders -1\n",
       "23446581                                    Harrison_Barnes  0\n",
       "166610                                 List_of_micronations  0\n",
       "29202630                                 Tyler,_The_Creator  0\n",
       "37261                                         Ion_Antonescu -1\n",
       "16243                                      Jawaharlal_Nehru  0\n",
       "27159                                       Sherlock_Holmes  0\n",
       "1616492                                       Internet_meme -1\n",
       "2504                                            Amphetamine  0\n",
       "13097064                         Harold_Pinter_bibliography -1\n",
       "25762                                    Russian_Revolution  0\n",
       "683122                                                Bambi  0\n",
       "5326967                     Maurice_Roche,_4th_Baron_Fermoy  0\n",
       "19019                                                 Mr._T  0\n",
       "1116229                          Genesis_creation_narrative -1\n",
       "1326645                                     Chemical_change -1\n",
       "39837608                                             Artpop  0\n",
       "458690                                     Allan_R._Bomhard  0\n",
       "55523                                              R._Kelly  0\n",
       "24176643                                     Mirror_Monster  0\n",
       "...                                                     ... ..\n",
       "12992431                                             KidsCo -1\n",
       "10284103                                    House_of_Merode -1\n",
       "65220                                      Nagorno-Karabakh  0\n",
       "2407865                                                Matt -1\n",
       "18389770                           List_of_Bratz_characters  0\n",
       "9877706                                      Emanuel_School  0\n",
       "45397                                          Jane_Goodall -1\n",
       "5118299                                             Fulgore  0\n",
       "5023307                                      TNT_(magazine)  0\n",
       "38103                                                   BMX -1\n",
       "548669                                Demolition_Man_(film)  0\n",
       "5985      Canadian_Radio-television_and_Telecommunicatio... -1\n",
       "13629643       Danville_Public_Library_(Danville,_Illinois) -1\n",
       "19196130    Nickelodeon_Australian_Kids'_Choice_Awards_2008  0\n",
       "415036                                        Jeremy_Corbyn  0\n",
       "57445                                     Thurgood_Marshall  0\n",
       "13383154                                   Javier_Hernández  0\n",
       "18319111                                      Shilpa_Shetty  0\n",
       "24194794                                 Amber_Liu_(singer)  0\n",
       "2715536                                       Víctor_Valdés  0\n",
       "32864                                           WikiWikiWeb -1\n",
       "55490                                            Roald_Dahl  0\n",
       "22992135                                  Rapunzel_(Disney)  0\n",
       "55212                               Newton's_laws_of_motion -1\n",
       "8531                                                 Dragon -1\n",
       "350924                                          Adam_Goodes  0\n",
       "3978                                         Book_of_Mormon  0\n",
       "298599                                Blackburn_Rovers_F.C.  0\n",
       "6924584                                            Predacon -1\n",
       "45627436                                        Bunny_Meyer  0\n",
       "2418326                                          Afzal_Guru -1\n",
       "29635016                         Teen_Wolf_(2011_TV_series)  0\n",
       "929                                                   Alpha  0\n",
       "2828209                                   Tuʻi_Tonga_Empire  0\n",
       "1351301                                    Rolie_Polie_Olie  0\n",
       "19527                                            Mao_Zedong  0\n",
       "186300                                  John_D._Rockefeller -1\n",
       "1354872                                    Dick_Strawbridge  0\n",
       "5118400                      Black_Orchid_(Killer_Instinct)  0\n",
       "6341469                                          Pedophilia  0\n",
       "36599245                                     Scott_Flanigan  0\n",
       "12120000                                            27_Club -1\n",
       "83019                                          Motherfucker -1\n",
       "81083                                         Davy_Crockett  0\n",
       "21552009                                              Aisha  0\n",
       "47125                                          Easter_Bunny -1\n",
       "10483209                                      Skilled_Group -1\n",
       "33947707                                         Ross_Lynch  0\n",
       "33591033                                   Parineeti_Chopra  0\n",
       "405134                                         Lot_in_Islam -1\n",
       "\n",
       "[7489 rows x 2 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
