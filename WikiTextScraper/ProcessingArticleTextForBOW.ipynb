{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "epa_ft_url = \"https://storage.googleapis.com/atriskwiki/epa_fulltext.txt\"\n",
    "nepa_ft_url = \"https://storage.googleapis.com/atriskwiki/nepa_fulltext.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\",25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ScrapeAndStemIntros(fulltextsURL):\n",
    "    \"\"\"Retrieves JSON-formatted article fulltexts from the specified URL and returns a pandas.DataFrame\n",
    "    containing stemmed introduction strings for each article, indexed by article_id\n",
    "    \"\"\"\n",
    "    print(\"Getting remote file...\")\n",
    "    fulltexts = requests.get(fulltextsURL).text.split(\"\\n\")\n",
    "    print(\"File retrieved and split!\")\n",
    "    processedTexts = dict()\n",
    "    \n",
    "    print(\"Stemming records...\")\n",
    "    for (ix, ft) in enumerate(fulltexts):\n",
    "        if (ix + 1) % 500 == 0:\n",
    "            print(\"Stemming record {}.\".format(ix + 1))\n",
    "        try:\n",
    "            ft_json = json.loads(ft)\n",
    "            article_id = list(ft_json.keys())[0]\n",
    "            processedTexts[article_id] = GetAndCleanArticle(ft_json[article_id])\n",
    "        except:\n",
    "            print(\"RECORD FAILED: {} {}\".format(article_id,ft))\n",
    "    \n",
    "    df_intro = pd.DataFrame.from_dict(processedTexts, orient=\"index\")\n",
    "    df_intro.index.name = 'article_id'\n",
    "    df_intro.columns = ['intro_stems']\n",
    "    \n",
    "    return df_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetAndCleanArticle(text, fulltext=False):\n",
    "    \"\"\"Expects a string containing a wikipedia article formatted with explaintext and exsectionformat=wiki\n",
    "    Returns a list containing all Porter stemmed non-stop words from the introductory section of the article\n",
    "    (can be set to process the entire article with fulltext=True)\"\"\"\n",
    "    if not fulltext:\n",
    "        intro = re.compile(\"==.*?==\").split(text)[0]\n",
    "    else:\n",
    "        intro = text\n",
    "    intro = re.sub(r\"\\n\",\" \",intro)\n",
    "    intro = re.sub(r\"[^A-Za-z ]\",\"\",intro)\n",
    "    intro = intro.lower()\n",
    "    intro_words = intro.split()\n",
    "    intro_words = [stemmer.stem(w) for w in intro_words if w not in stops]\n",
    "    return \" \".join(intro_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def VectorizeMostCommonFeatures(dataframe, num_features):\n",
    "    \"\"\"Identifies the num_features most common features from a dataframe generated by ScrapeAndStemIntros\n",
    "    containing space-delimited stemmed strings.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features = num_features)\n",
    "    vec_words = vectorizer.fit_transform(dataframe[\"ft_stems\"])\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    count_vocab = vec_words.toarray()\n",
    "    count_df = pd.DataFrame(count_vocab, columns=vocab, index=dataframe.index)\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VectorizeVocabFeatures(dataframe, vocab):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, vocabulary=vocab)\n",
    "    vec_words = vectorizer.fit_transform(dataframe[\"ft_stems\"])\n",
    "    features = vectorizer.get_feature_names()\n",
    "    count_vocab = vec_words.toarray()\n",
    "    return pd.DataFrame(count_vocab, columns=features, index=dataframe.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ProcessWordOccurrences(vec_count_df):\n",
    "    '''Given a dataframe (rows: articles, cols: word counts) containg the output of VectorizeMostCommonFeatures, output a dataframe\n",
    "    containing the number of occurrences of each word across the dataset, the number of articles with at least\n",
    "    one occurrence of the word, and the fraction of articles with at least once occurrence.'''\n",
    "    vocab = vec_count_df.columns.values\n",
    "    count_by_word = np.sum(vec_count_df, axis=0)\n",
    "    \n",
    "    present = vec_count_df > np.zeros(vec_count_df.shape)\n",
    "    present_by_word = np.sum(present, axis=0)\n",
    "    present_by_word_frac = present_by_word / vec_count_df.shape[0]\n",
    "    \n",
    "    df = pd.concat([count_by_word,present_by_word,present_by_word_frac],axis=1)\n",
    "    df.columns = ['occurences','articles','frac_of_articles']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def StemFTfromCSV(csv_fn):\n",
    "    now = datetime.datetime.now().time()\n",
    "    print(\"[{:02d}:{:02d}:{:02d}] Opening file: {}\".format(now.hour,now.minute,now.second,csv_fn))\n",
    "    processedTexts = dict()\n",
    "    with open(csv_fn,\"r\") as f:\n",
    "        for ix, line in enumerate(f):\n",
    "            if (ix + 1) % 1000 == 0:\n",
    "                now = datetime.datetime.now().time()\n",
    "                print(\"[{:02d}:{:02d}:{:02d}] Processing record {}\".format(now.hour,now.minute,now.second,ix+1))\n",
    "            try:\n",
    "                ft_json = json.loads(line)\n",
    "                article_id = list(ft_json.keys())[0]\n",
    "                processedTexts[article_id] = GetAndCleanArticle(ft_json[article_id],fulltext=True)\n",
    "            except Exception as e:\n",
    "                now = datetime.datetime.now().time()\n",
    "                print(\"[{:02d}:{:02d}:{:02d}] {}:\\n{}\".format(now.hour,now.minute,now.second,e,line[:140]))\n",
    "            \n",
    "    df_ft = pd.DataFrame.from_dict(processedTexts, orient=\"index\")\n",
    "    df_ft.index.name = 'article_id'\n",
    "    df_ft.columns = ['ft_stems']\n",
    "            \n",
    "    return df_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetFTLength(fulltext):\n",
    "    fulltext = re.sub(r\"\\n\",\" \",fulltext)\n",
    "    fulltext = re.sub(r\"[^A-Za-z ]\",\"\",fulltext)\n",
    "    return len(fulltext.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:09] Opening file: ArticleFulltexts/epa_fulltext.txt\n",
      "[14:46:55] Processing record 1000\n",
      "[14:47:25] Processing record 2000\n",
      "[14:47:46] Processing record 3000\n",
      "[14:48:00] Processing record 4000\n",
      "[14:48:17] Processing record 5000\n",
      "[14:48:33] Processing record 6000\n",
      "[14:48:45] Processing record 7000\n"
     ]
    }
   ],
   "source": [
    "stemmed_fts = StemFTfromCSV(\"ArticleFulltexts/epa_fulltext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:48:50] Opening file: ArticleFulltexts/071816_fulltext_batch1.txt\n",
      "[14:49:07] Processing record 1000\n",
      "[14:49:21] Processing record 2000\n",
      "[14:49:30] Processing record 3000\n",
      "[14:49:39] Processing record 4000\n",
      "[14:49:46] Processing record 5000\n",
      "[14:49:52] Processing record 6000\n",
      "[14:49:58] Processing record 7000\n",
      "[14:50:05] Processing record 8000\n",
      "[14:50:11] Processing record 9000\n",
      "[14:50:16] Opening file: ArticleFulltexts/071816_fulltext_batch2.txt\n",
      "[14:50:32] Processing record 1000\n",
      "[14:50:44] Processing record 2000\n",
      "[14:50:53] Processing record 3000\n",
      "[14:51:00] Processing record 4000\n",
      "[14:51:06] Processing record 5000\n",
      "[14:51:13] Processing record 6000\n",
      "[14:51:19] Processing record 7000\n",
      "[14:51:24] Processing record 8000\n",
      "[14:51:26] expected string or bytes-like object:\n",
      "{\"51074306\": null}\n",
      "\n",
      "[14:51:26] Opening file: ArticleFulltexts/071816_fulltext_batch3.txt\n",
      "[14:51:43] Processing record 1000\n",
      "[14:51:55] Processing record 2000\n",
      "[14:52:05] Processing record 3000\n",
      "[14:52:14] Processing record 4000\n",
      "[14:52:21] Processing record 5000\n",
      "[14:52:27] Processing record 6000\n",
      "[14:52:34] Processing record 7000\n",
      "[14:52:40] Processing record 8000\n",
      "[14:52:46] Processing record 9000\n",
      "[14:52:51] Opening file: ArticleFulltexts/071816_fulltext_batch4.txt\n",
      "[14:53:08] Processing record 1000\n",
      "[14:53:20] Processing record 2000\n",
      "[14:53:30] Processing record 3000\n",
      "[14:53:39] Processing record 4000\n",
      "[14:53:47] Processing record 5000\n",
      "[14:53:53] Processing record 6000\n",
      "[14:53:59] Processing record 7000\n",
      "[14:54:05] Processing record 8000\n",
      "[14:54:10] Processing record 9000\n",
      "[14:54:14] Opening file: ArticleFulltexts/071816_fulltext_batch5.txt\n",
      "[14:54:31] Processing record 1000\n",
      "[14:54:44] Processing record 2000\n",
      "[14:54:54] Processing record 3000\n",
      "[14:55:02] Processing record 4000\n",
      "[14:55:10] Processing record 5000\n",
      "[14:55:16] Processing record 6000\n",
      "[14:55:22] Processing record 7000\n",
      "[14:55:28] Processing record 8000\n",
      "[14:55:33] Processing record 9000\n",
      "[14:55:38] Opening file: ArticleFulltexts/071816_fulltext_batch6.txt\n",
      "[14:55:56] Processing record 1000\n",
      "[14:56:10] Processing record 2000\n",
      "[14:56:19] Processing record 3000\n",
      "[14:56:27] Processing record 4000\n",
      "[14:56:34] Processing record 5000\n",
      "[14:56:40] Processing record 6000\n",
      "[14:56:46] Processing record 7000\n",
      "[14:56:52] Processing record 8000\n",
      "[14:56:58] Processing record 9000\n",
      "[14:56:59] expected string or bytes-like object:\n",
      "{\"44972286\": null}\n",
      "\n",
      "[14:57:02] Opening file: ArticleFulltexts/071816_fulltext_batch7.txt\n",
      "[14:57:19] Processing record 1000\n",
      "[14:57:31] Processing record 2000\n",
      "[14:57:41] Processing record 3000\n",
      "[14:57:49] Processing record 4000\n",
      "[14:57:56] Processing record 5000\n",
      "[14:57:59] expected string or bytes-like object:\n",
      "{\"18076569\": null}\n",
      "\n",
      "[14:58:02] Processing record 6000\n",
      "[14:58:08] Processing record 7000\n",
      "[14:58:09] expected string or bytes-like object:\n",
      "{\"29310534\": null}\n",
      "\n",
      "[14:58:14] Processing record 8000\n",
      "[14:58:19] Processing record 9000\n",
      "[14:58:24] Opening file: ArticleFulltexts/071816_fulltext_batch8.txt\n",
      "[14:58:40] Processing record 1000\n",
      "[14:58:52] Processing record 2000\n",
      "[14:59:00] Processing record 3000\n",
      "[14:59:07] Processing record 4000\n",
      "[14:59:13] Processing record 5000\n",
      "[14:59:19] Processing record 6000\n",
      "[14:59:24] Processing record 7000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"ArticleFulltexts/\"\n",
    "for filename in os.listdir(path):\n",
    "    if filename == \"epa_fulltext.txt\":\n",
    "        pass\n",
    "    else:\n",
    "        stemmed = StemFTfromCSV(path + filename)\n",
    "        stemmed_fts = pd.concat([stemmed_fts, stemmed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articleLength = dict()\n",
    "for filename in os.listdir(path):\n",
    "    with open(path + filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                ft_json = json.loads(line)\n",
    "                article_id = list(ft_json.keys())[0]\n",
    "                articleLength[article_id] = GetFTLength(ft_json[article_id])\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_length = pd.DataFrame.from_dict(articleLength,orient='index')\n",
    "df_length.index.name = 'article_id'\n",
    "df_length.columns = ['article_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# your username and password go here!\n",
    "creds = {'user': 'brian', \n",
    "         'pswd': 'farley'} \n",
    "\n",
    "\n",
    "connection = pymysql.connect(host=\"atriskwiki.cbprs6vpqcbz.us-west-1.rds.amazonaws.com\",\n",
    "                            user=creds['user'],\n",
    "                            password=creds['pswd'],\n",
    "                            port=3306,\n",
    "                            db=\"atriskwiki\",\n",
    "                            charset=\"utf8\")\n",
    "\n",
    "cur = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_is_ep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101162</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101460</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102110</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102592</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51103467</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105617</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105871</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106337</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106355</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106579</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51107865</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83301 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            page_is_ep\n",
       "article_id            \n",
       "25                   1\n",
       "290                  1\n",
       "307                  1\n",
       "308                  1\n",
       "316                  0\n",
       "572                  0\n",
       "594                  1\n",
       "615                  0\n",
       "620                  1\n",
       "621                  0\n",
       "624                  1\n",
       "628                  1\n",
       "...                ...\n",
       "51101162             0\n",
       "51101460             0\n",
       "51102110             0\n",
       "51102592             0\n",
       "51103467             0\n",
       "51105617             0\n",
       "51105871             0\n",
       "51106337             0\n",
       "51106355             0\n",
       "51106579             0\n",
       "51106996             0\n",
       "51107865             0\n",
       "\n",
       "[83301 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idQ = 'SELECT page_id, page_is_ep FROM core WHERE page_batch < 99'\n",
    "cur.execute(idQ)\n",
    "\n",
    "id_and_ep = pd.DataFrame(list(cur.fetchall()),columns=['article_id','page_is_ep'])\n",
    "id_and_ep.index = id_and_ep['article_id']\n",
    "del id_and_ep['article_id']\n",
    "id_and_ep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_stems</th>\n",
       "      <th>article_words</th>\n",
       "      <th>page_is_ep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>autism neurodevelopment disord character impai...</td>\n",
       "      <td>7674</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>name e plural ae first letter first vowel iso ...</td>\n",
       "      <td>1732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>abraham lincoln ebrhm lkn februari april th pr...</td>\n",
       "      <td>15026</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>aristotl rsttl greek aristotl aristotl bc gree...</td>\n",
       "      <td>10815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>academi award best product design recogn achie...</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>agricultur scienc broad multidisciplinari fiel...</td>\n",
       "      <td>1310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>apollo attic ionic homer greek apolln gen dori...</td>\n",
       "      <td>11769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>american footbal confer afc one two confer nat...</td>\n",
       "      <td>722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>anim farm allegor dystopian novella georg orwe...</td>\n",
       "      <td>6092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>amphibian ectotherm tetrapod vertebr class amp...</td>\n",
       "      <td>11694</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>alaska lsk us state situat northwest extrem am...</td>\n",
       "      <td>9800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>aldou leonard huxley ld hksli juli novemb engl...</td>\n",
       "      <td>3578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101162</th>\n",
       "      <td>jonathan pie fiction news report creat play ac...</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101460</th>\n",
       "      <td>fem televis channel oper bermuda compani centr...</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102110</th>\n",
       "      <td>list archer particip countri summer olymp rio ...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102592</th>\n",
       "      <td>cal tjaderstan getz sextet album vibraphonist ...</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51103467</th>\n",
       "      <td>addi villag nakodar jalandhar district punjab ...</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105617</th>\n",
       "      <td>sieg arrah eightdaylong defenc fortifi residen...</td>\n",
       "      <td>2741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105871</th>\n",
       "      <td>season galatasaray th exist rd consecut season...</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106337</th>\n",
       "      <td>pablo andrea mexican children telenovela produ...</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106355</th>\n",
       "      <td>ilmi parkkari finnish film stage actress selec...</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106579</th>\n",
       "      <td>page swan swan design ron holland built nautor...</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106996</th>\n",
       "      <td>list aircraft numer order manufactur follow al...</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51107865</th>\n",
       "      <td>young love song canadian singer nikki yanofski...</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83297 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     ft_stems  article_words  \\\n",
       "article_id                                                                     \n",
       "25          autism neurodevelopment disord character impai...           7674   \n",
       "290         name e plural ae first letter first vowel iso ...           1732   \n",
       "307         abraham lincoln ebrhm lkn februari april th pr...          15026   \n",
       "308         aristotl rsttl greek aristotl aristotl bc gree...          10815   \n",
       "316         academi award best product design recogn achie...            143   \n",
       "572         agricultur scienc broad multidisciplinari fiel...           1310   \n",
       "594         apollo attic ionic homer greek apolln gen dori...          11769   \n",
       "615         american footbal confer afc one two confer nat...            722   \n",
       "620         anim farm allegor dystopian novella georg orwe...           6092   \n",
       "621         amphibian ectotherm tetrapod vertebr class amp...          11694   \n",
       "624         alaska lsk us state situat northwest extrem am...           9800   \n",
       "628         aldou leonard huxley ld hksli juli novemb engl...           3578   \n",
       "...                                                       ...            ...   \n",
       "51101162    jonathan pie fiction news report creat play ac...            115   \n",
       "51101460    fem televis channel oper bermuda compani centr...             81   \n",
       "51102110    list archer particip countri summer olymp rio ...             55   \n",
       "51102592    cal tjaderstan getz sextet album vibraphonist ...            110   \n",
       "51103467    addi villag nakodar jalandhar district punjab ...            104   \n",
       "51105617    sieg arrah eightdaylong defenc fortifi residen...           2741   \n",
       "51105871    season galatasaray th exist rd consecut season...            126   \n",
       "51106337    pablo andrea mexican children telenovela produ...            169   \n",
       "51106355    ilmi parkkari finnish film stage actress selec...             47   \n",
       "51106579    page swan swan design ron holland built nautor...             53   \n",
       "51106996    list aircraft numer order manufactur follow al...           1965   \n",
       "51107865    young love song canadian singer nikki yanofski...             93   \n",
       "\n",
       "            page_is_ep  \n",
       "article_id              \n",
       "25                   1  \n",
       "290                  1  \n",
       "307                  1  \n",
       "308                  1  \n",
       "316                  0  \n",
       "572                  0  \n",
       "594                  1  \n",
       "615                  0  \n",
       "620                  1  \n",
       "621                  0  \n",
       "624                  1  \n",
       "628                  1  \n",
       "...                ...  \n",
       "51101162             0  \n",
       "51101460             0  \n",
       "51102110             0  \n",
       "51102592             0  \n",
       "51103467             0  \n",
       "51105617             0  \n",
       "51105871             0  \n",
       "51106337             0  \n",
       "51106355             0  \n",
       "51106579             0  \n",
       "51106996             0  \n",
       "51107865             0  \n",
       "\n",
       "[83297 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_length.index = df_length.index.astype(int)\n",
    "stemmed_fts.index = stemmed_fts.index.astype(int)\n",
    "stemmed_with_info = stemmed_fts.merge(df_length.merge(id_and_ep, how='inner', left_index=True, right_index=True),how='inner',left_index=True, right_index=True)\n",
    "stemmed_with_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_vec_count = VectorizeMostCommonFeatures(stemmed_with_info, 10000)\n",
    "epa_vec_count = VectorizeMostCommonFeatures(stemmed_with_info[stemmed_with_info['page_is_ep'] == 1],10000)\n",
    "samp_vec_count = VectorizeMostCommonFeatures(stemmed_with_info[stemmed_with_info['page_is_ep'] == 0],10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = set(total_vec_count.columns)\n",
    "epa_words = set(epa_vec_count.columns)\n",
    "samp_words = set(samp_vec_count.columns)\n",
    "\n",
    "epa_only = epa_words - samp_words\n",
    "samp_only = samp_words - epa_words\n",
    "union_words = epa_words | samp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1715\n",
      "1715\n",
      "11715\n"
     ]
    }
   ],
   "source": [
    "print(len(epa_only))\n",
    "print(len(samp_only))\n",
    "print(len(union_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_sum = total_vec_count.sum(axis=0)\n",
    "epa_sum = epa_vec_count.sum(axis=0)\n",
    "samp_sum = samp_vec_count.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scientolog           2692\n",
       "peni                  884\n",
       "cena                  761\n",
       "wrestlemania          750\n",
       "megatron              742\n",
       "cannabi               694\n",
       "sperm                 652\n",
       "jihad                 630\n",
       "sharif                617\n",
       "beckham               612\n",
       "orton                 611\n",
       "zimmerman             608\n",
       "                     ... \n",
       "kyli                  121\n",
       "protoindoeuropean     121\n",
       "agrarian              121\n",
       "sylvest               121\n",
       "dumbo                 121\n",
       "shnen                 121\n",
       "jafar                 121\n",
       "theropod              121\n",
       "dhoom                 121\n",
       "donni                 121\n",
       "purif                 121\n",
       "dutt                  121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa_sum[epa_sum.index.isin(epa_only)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lc              4429\n",
       "terminu         2360\n",
       "nonfamili       2313\n",
       "freeway         2239\n",
       "mathbf          2233\n",
       "somerset        1931\n",
       "tram            1900\n",
       "saskatchewan    1800\n",
       "turret          1800\n",
       "duchi           1792\n",
       "durham          1705\n",
       "brighton        1703\n",
       "                ... \n",
       "woodward         454\n",
       "fran             454\n",
       "collier          454\n",
       "tuvalu           454\n",
       "atkin            454\n",
       "cheng            453\n",
       "slovenian        453\n",
       "derrick          453\n",
       "multipurpos      453\n",
       "nola             453\n",
       "quarrel          453\n",
       "godfrey          453\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_sum[samp_sum.index.isin(samp_only)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union_vec_count = VectorizeVocabFeatures(stemmed_with_info,union_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(union_vec_count['sperm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union_present = union_vec_count > np.zeros_like(union_vec_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aarhu</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbey</th>\n",
       "      <th>...</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoolog</th>\n",
       "      <th>zoroastrian</th>\n",
       "      <th>zrich</th>\n",
       "      <th>zu</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zur</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101162</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101460</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102110</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102592</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51103467</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105617</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105871</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106337</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106355</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106579</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106996</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51107865</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83297 rows × 11715 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               aa    aaa aachen aaliyah  aarhu  aaron     ab abandon   abba  \\\n",
       "article_id                                                                    \n",
       "25          False  False  False   False  False  False  False   False  False   \n",
       "290          True  False  False   False  False   True  False   False  False   \n",
       "307         False  False  False   False  False  False  False    True  False   \n",
       "308         False  False  False   False  False  False  False   False  False   \n",
       "316         False  False  False   False  False  False  False   False  False   \n",
       "572         False  False  False   False  False  False  False   False  False   \n",
       "594         False  False  False   False  False  False  False    True  False   \n",
       "615         False  False  False   False  False  False  False   False  False   \n",
       "620         False  False  False   False  False  False  False    True  False   \n",
       "621         False  False  False   False  False  False  False   False  False   \n",
       "624         False  False  False   False  False  False  False    True  False   \n",
       "628         False  False  False   False  False  False  False   False  False   \n",
       "...           ...    ...    ...     ...    ...    ...    ...     ...    ...   \n",
       "51101162    False  False  False   False  False  False  False   False  False   \n",
       "51101460    False  False  False   False  False  False  False   False  False   \n",
       "51102110    False  False  False   False  False  False  False   False  False   \n",
       "51102592    False  False  False   False  False  False  False   False  False   \n",
       "51103467    False  False  False   False  False  False  False   False  False   \n",
       "51105617    False  False  False   False  False  False  False    True  False   \n",
       "51105871    False  False  False   False  False  False  False   False  False   \n",
       "51106337    False  False  False   False  False  False  False   False  False   \n",
       "51106355    False  False  False   False  False  False  False   False  False   \n",
       "51106579    False  False  False   False  False  False  False   False  False   \n",
       "51106996     True  False  False   False  False  False  False   False  False   \n",
       "51107865    False  False  False   False  False  False  False   False  False   \n",
       "\n",
       "            abbey  ...    zombi   zone    zoo zoolog zoroastrian  zrich  \\\n",
       "article_id         ...                                                    \n",
       "25          False  ...    False  False  False  False       False  False   \n",
       "290         False  ...    False  False  False  False       False  False   \n",
       "307         False  ...    False  False  False  False       False  False   \n",
       "308         False  ...    False  False  False   True       False  False   \n",
       "316         False  ...    False  False  False  False       False  False   \n",
       "572         False  ...    False  False  False  False       False  False   \n",
       "594         False  ...    False  False  False  False       False  False   \n",
       "615         False  ...    False  False  False  False       False  False   \n",
       "620         False  ...    False  False  False  False       False  False   \n",
       "621         False  ...    False   True   True   True       False  False   \n",
       "624         False  ...    False   True  False  False       False  False   \n",
       "628         False  ...    False  False  False  False       False  False   \n",
       "...           ...  ...      ...    ...    ...    ...         ...    ...   \n",
       "51101162    False  ...    False  False  False  False       False  False   \n",
       "51101460    False  ...    False  False  False  False       False  False   \n",
       "51102110    False  ...    False  False  False  False       False  False   \n",
       "51102592    False  ...    False  False  False  False       False  False   \n",
       "51103467    False  ...    False  False  False  False       False  False   \n",
       "51105617    False  ...    False  False  False  False       False  False   \n",
       "51105871    False  ...    False  False  False  False       False  False   \n",
       "51106337    False  ...    False  False  False  False       False  False   \n",
       "51106355    False  ...    False  False  False  False       False  False   \n",
       "51106579    False  ...    False  False  False  False       False  False   \n",
       "51106996    False  ...    False  False  False  False       False  False   \n",
       "51107865    False  ...    False  False  False  False       False  False   \n",
       "\n",
       "               zu zuckerberg    zur zurich  \n",
       "article_id                                  \n",
       "25          False      False  False  False  \n",
       "290         False      False  False  False  \n",
       "307         False      False  False  False  \n",
       "308         False      False  False  False  \n",
       "316         False      False  False  False  \n",
       "572         False      False  False  False  \n",
       "594         False      False  False  False  \n",
       "615         False      False  False  False  \n",
       "620         False      False  False  False  \n",
       "621         False      False  False  False  \n",
       "624         False      False  False  False  \n",
       "628         False      False  False  False  \n",
       "...           ...        ...    ...    ...  \n",
       "51101162    False      False  False  False  \n",
       "51101460    False      False  False  False  \n",
       "51102110    False      False  False  False  \n",
       "51102592    False      False  False  False  \n",
       "51103467    False      False  False  False  \n",
       "51105617    False      False  False  False  \n",
       "51105871    False      False  False  False  \n",
       "51106337    False      False  False  False  \n",
       "51106355    False      False  False  False  \n",
       "51106579    False      False  False  False  \n",
       "51106996    False      False  False  False  \n",
       "51107865    False      False  False  False  \n",
       "\n",
       "[83297 rows x 11715 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union_pres_freq = union_present.sum(axis=0) / len(union_present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_ids = stemmed_with_info[stemmed_with_info['page_is_ep'] == 1].index\n",
    "samp_ids = stemmed_with_info[stemmed_with_info['page_is_ep'] == 0].index\n",
    "union_pres_freq_ep = union_present[union_present.index.isin(ep_ids)].sum(axis=0) / len(ep_ids)\n",
    "union_pres_freq_sample = union_present[union_present.index.isin(samp_ids)].sum(axis=0) / len(samp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "refer          0.940872\n",
       "also           0.880813\n",
       "extern         0.785012\n",
       "link           0.781424\n",
       "one            0.777438\n",
       "first          0.761892\n",
       "includ         0.737842\n",
       "time           0.722695\n",
       "two            0.714855\n",
       "new            0.714457\n",
       "year           0.708743\n",
       "state          0.689476\n",
       "                 ...   \n",
       "espuela        0.000266\n",
       "rq             0.000266\n",
       "tq             0.000133\n",
       "xaviax         0.000133\n",
       "gmina          0.000133\n",
       "sogod          0.000133\n",
       "mathcal        0.000133\n",
       "seenment       0.000133\n",
       "metald         0.000133\n",
       "hbner          0.000000\n",
       "schiffermul    0.000000\n",
       "anoli          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_pres_freq_ep.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "refer       0.907735\n",
       "also        0.720619\n",
       "link        0.640839\n",
       "extern      0.632947\n",
       "first       0.607831\n",
       "one         0.587718\n",
       "two         0.543440\n",
       "year        0.539124\n",
       "includ      0.530137\n",
       "time        0.517507\n",
       "new         0.514564\n",
       "state       0.467646\n",
       "              ...   \n",
       "sharpay     0.000040\n",
       "espuela     0.000026\n",
       "lulzsec     0.000026\n",
       "castiel     0.000026\n",
       "xaviax      0.000013\n",
       "nyit        0.000013\n",
       "gamerg      0.000013\n",
       "metald      0.000013\n",
       "sogod       0.000013\n",
       "shaider     0.000000\n",
       "spielban    0.000000\n",
       "seenment    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_pres_freq_sample.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_present = pd.concat([union_pres_freq,union_pres_freq_ep,union_pres_freq_sample],axis=1)\n",
    "vocab_present.columns=['all','ep','sample']\n",
    "vocab_present['fold_ep'] = vocab_present['ep'] / vocab_present['sample']\n",
    "vocab_present['fold_ep'].sort_values(ascending=False)\n",
    "vocab_present.to_csv(path_or_buf=\"union_vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([      25,      308,      624,      736,      737,      783,\n",
       "                 800,      863,     1537,     1806,\n",
       "            ...\n",
       "            26807275, 28249265, 28329803, 37729052, 40017873, 40817590,\n",
       "            41688778, 44275267, 45645094, 47398645],\n",
       "           dtype='int64', name='article_id', length=440)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_present[(union_present['dmoz'] > 0) & union_present.index.isin(ep_ids)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uvc_normed_to_length = union_vec_count.divide(stemmed_with_info['article_words'], axis='rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index          666376\n",
       "aa             666376\n",
       "aaa            666376\n",
       "aachen         666376\n",
       "aaliyah        666376\n",
       "aarhu          666376\n",
       "aaron          666376\n",
       "ab             666376\n",
       "abandon        666376\n",
       "abba           666376\n",
       "abbey          666376\n",
       "abbot          666376\n",
       "                ...  \n",
       "zodiac         666376\n",
       "zoe            666376\n",
       "zombi          666376\n",
       "zone           666376\n",
       "zoo            666376\n",
       "zoolog         666376\n",
       "zoroastrian    666376\n",
       "zrich          666376\n",
       "zu             666376\n",
       "zuckerberg     666376\n",
       "zur            666376\n",
       "zurich         666376\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uvc_normed_to_length.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse_up = union_present.to_sparse(fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index          666376\n",
       "aa               6520\n",
       "aaa              3080\n",
       "aachen            992\n",
       "aaliyah           408\n",
       "aarhu             640\n",
       "aaron           10952\n",
       "ab               9080\n",
       "abandon         45848\n",
       "abba             3296\n",
       "abbey            9384\n",
       "abbot            3696\n",
       "                ...  \n",
       "zodiac           1528\n",
       "zoe              1800\n",
       "zombi            3560\n",
       "zone            36080\n",
       "zoo              5600\n",
       "zoolog           3120\n",
       "zoroastrian      1144\n",
       "zrich            2312\n",
       "zu               2784\n",
       "zuckerberg        256\n",
       "zur              3600\n",
       "zurich           2600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse_untl = uvc_normed_to_length.to_sparse(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(sparse_up,open(\"sparse_up.p\",\"wb\"))\n",
    "pickle.dump(sparse_untl,open(\"sparse_untl.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "sparse_up = pickle.load(open(\"sparse_up.p\",\"rb\"))\n",
    "sparse_untl = pickle.load(open(\"sparse_untl.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fmtTS():\n",
    "    now = datetime.datetime.now().time()\n",
    "    return \"[{:02d}:{:02d}:{:02d}]\".format(now.hour,now.minute,now.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#union_vec_count.to_csv(path_or_buf=\"fulltext_union_vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_count_all = sparse_untl.mean(axis=0)\n",
    "norm_count_ep = sparse_untl[stemmed_with_info['page_is_ep'] == 1].mean(axis=0)\n",
    "norm_count_samp = sparse_untl[stemmed_with_info['page_is_ep'] == 0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normed_words = pd.concat([norm_count_all,norm_count_ep,norm_count_samp],axis=1)\n",
    "normed_words.columns = ['all_mean_norm','ep_mean_norm','samp_mean_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normed_words.to_csv(path_or_buf=\"length_normalized_vec_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union_present_cols = [x + \"_p\" for x in sparse_up.columns]\n",
    "union_norm_cols = [x + \"_n\" for x in sparse_untl.columns]\n",
    "article_index = sparse_up.index.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union_norm_scaler = sparse_untl.mean(axis=0)\n",
    "scaled_sparse_untl = sparse_untl.divide(union_norm_scaler,axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_p</th>\n",
       "      <th>aaa_p</th>\n",
       "      <th>aachen_p</th>\n",
       "      <th>aaliyah_p</th>\n",
       "      <th>aarhu_p</th>\n",
       "      <th>aaron_p</th>\n",
       "      <th>ab_p</th>\n",
       "      <th>abandon_p</th>\n",
       "      <th>abba_p</th>\n",
       "      <th>abbey_p</th>\n",
       "      <th>...</th>\n",
       "      <th>zombi_n</th>\n",
       "      <th>zone_n</th>\n",
       "      <th>zoo_n</th>\n",
       "      <th>zoolog_n</th>\n",
       "      <th>zoroastrian_n</th>\n",
       "      <th>zrich_n</th>\n",
       "      <th>zu_n</th>\n",
       "      <th>zuckerberg_n</th>\n",
       "      <th>zur_n</th>\n",
       "      <th>zurich_n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101162</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51101460</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51102592</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51103467</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105617</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51105871</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106337</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106355</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106579</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51106996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51107865</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83297 rows × 23430 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            aa_p  aaa_p  aachen_p  aaliyah_p  aarhu_p  aaron_p  ab_p  \\\n",
       "article_id                                                             \n",
       "25           0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "290          1.0    0.0       0.0        0.0      0.0      1.0   0.0   \n",
       "307          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "308          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "316          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "572          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "594          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "615          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "620          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "621          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "624          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "628          0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "...          ...    ...       ...        ...      ...      ...   ...   \n",
       "51101162     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51101460     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51102110     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51102592     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51103467     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51105617     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51105871     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51106337     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51106355     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51106579     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51106996     1.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "51107865     0.0    0.0       0.0        0.0      0.0      0.0   0.0   \n",
       "\n",
       "            abandon_p  abba_p  abbey_p    ...     zombi_n    zone_n     zoo_n  \\\n",
       "article_id                                ...                                   \n",
       "25                0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "290               0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "307               1.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "308               0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "316               0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "572               0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "594               1.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "615               0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "620               1.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "621               0.0     0.0      0.0    ...         0.0  0.001311  0.001534   \n",
       "624               1.0     0.0      0.0    ...         0.0  0.000782  0.000000   \n",
       "628               0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "...               ...     ...      ...    ...         ...       ...       ...   \n",
       "51101162          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51101460          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51102110          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51102592          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51103467          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51105617          1.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51105871          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51106337          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51106355          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51106579          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51106996          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "51107865          0.0     0.0      0.0    ...         0.0  0.000000  0.000000   \n",
       "\n",
       "            zoolog_n  zoroastrian_n  zrich_n  zu_n  zuckerberg_n  zur_n  \\\n",
       "article_id                                                                \n",
       "25          0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "290         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "307         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "308         0.020589            0.0      0.0   0.0           0.0    0.0   \n",
       "316         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "572         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "594         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "615         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "620         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "621         0.003174            0.0      0.0   0.0           0.0    0.0   \n",
       "624         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "628         0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "...              ...            ...      ...   ...           ...    ...   \n",
       "51101162    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51101460    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51102110    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51102592    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51103467    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51105617    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51105871    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51106337    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51106355    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51106579    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51106996    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "51107865    0.000000            0.0      0.0   0.0           0.0    0.0   \n",
       "\n",
       "            zurich_n  \n",
       "article_id            \n",
       "25               0.0  \n",
       "290              0.0  \n",
       "307              0.0  \n",
       "308              0.0  \n",
       "316              0.0  \n",
       "572              0.0  \n",
       "594              0.0  \n",
       "615              0.0  \n",
       "620              0.0  \n",
       "621              0.0  \n",
       "624              0.0  \n",
       "628              0.0  \n",
       "...              ...  \n",
       "51101162         0.0  \n",
       "51101460         0.0  \n",
       "51102110         0.0  \n",
       "51102592         0.0  \n",
       "51103467         0.0  \n",
       "51105617         0.0  \n",
       "51105871         0.0  \n",
       "51106337         0.0  \n",
       "51106355         0.0  \n",
       "51106579         0.0  \n",
       "51106996         0.0  \n",
       "51107865         0.0  \n",
       "\n",
       "[83297 rows x 23430 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features = pd.concat([sparse_up,scaled_sparse_untl],axis=1,join_axes=[sparse_up.index])\n",
    "word_features.columns = union_present_cols + union_norm_cols\n",
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     594,     1241,     1520,     2166,     5995,     6546,\n",
       "                7085,     7397,     8439,     9322,\n",
       "            ...\n",
       "            44161018, 47005220, 47351793, 47523415, 47895728, 48544304,\n",
       "            49999342, 50606473, 50901276, 50952770],\n",
       "           dtype='int64', name='article_id', length=481)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features.index[word_features['disambigu_p'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#necessary because a small number of articles have no article text\n",
    "word_features.dropna(axis='index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(word_features,open(\"word_features.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QforTrain = \"SELECT page_id,page_is_ep FROM core WHERE page_in_train = 1 AND page_batch < 99\"\n",
    "cur.execute(QforTrain)\n",
    "\n",
    "infoForTrain = pd.DataFrame(list(cur.fetchall()),columns=['page_id','page_is_ep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wf_y = infoForTrain[infoForTrain['page_id'].isin(word_features.index)]['page_is_ep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wf_train = word_features[word_features.index.isin(infoForTrain['page_id'])].copy()\n",
    "features = list(wf_train.columns)\n",
    "pages = list(wf_train.index)\n",
    "wf_for_fit = wf_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62453, 23430)\n",
      "(62453,)\n"
     ]
    }
   ],
   "source": [
    "print(wf_for_fit.shape)\n",
    "print(wf_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "\n",
    "lr_classifier = LogisticRegression(C=0.01,penalty='l1')\n",
    "lr_classifier.fit(wf_for_fit,wf_y)\n",
    "#scores = cross_validation.cross_val_score(lr_classifier, wf_for_fit, wf_y, cv=10)\n",
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96     56794\n",
      "          1       0.75      0.14      0.24      5659\n",
      "\n",
      "avg / total       0.91      0.92      0.89     62453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lr_classifier.predict(wf_for_fit)\n",
    "probs = lr_classifier.predict_proba(wf_for_fit)\n",
    "print(metrics.classification_report(wf_y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963892847421\n",
      "0.0906121403295\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(wf_y,predicted))\n",
    "print(wf_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(lr_classifier.coef_.T,columns=['lr_coef'])\n",
    "coefs = pd.concat([coefs,pd.Series(features)],axis=1)\n",
    "coefs.to_csv(path_or_buf=\"first_pass_lr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_vec_count = VectorizeMostCommonFeatures(epa_stem_intros, 5000)\n",
    "nepa_vec_count = VectorizeMostCommonFeatures(nepa_stem_intros, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_entries_with_words = epa_vec_count[epa_vec_count.sum(axis=1) > 0].index\n",
    "epa_vecs_with_words = epa_vec_count[epa_vec_count.sum(axis=1) > 0].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = np.sum(epa_vec_count, axis=0)\n",
    "epa_vec_count.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_occurrences = ProcessWordOccurrences(epa_vec_count)\n",
    "nepa_occurrences = ProcessWordOccurrences(nepa_vec_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occ = pd.merge(epa_occurrences, nepa_occurrences, left_index=True, right_index=True, suffixes=('_epa','_nepa'), indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occ.to_csv(path_or_buf=\"BOW_5000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "occ_all = pd.merge(epa_occurences, nepa_occurrences, how=\"outer\", left_index=True, right_index=True, suffixes=('_epa','_nepa'), indicator=True)\n",
    "#occ_all.to_csv(path_or_buf=\"BOW_5000_ALL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UnitVector(vector):\n",
    "    return vector/np.linalg.norm(vector)\n",
    "\n",
    "def AngleBetween(v1, v2):\n",
    "    v1_u = UnitVector(v1)\n",
    "    v2_u = UnitVector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_norms = np.linalg.norm(epa_vecs_with_words, axis=1) #calculates norms of row vectors\n",
    "epa_vecs_normed = epa_vecs_with_words / epa_norms[:,None] #unitizes row vectors\n",
    "distances = 180 * np.arccos(np.clip(np.dot(epa_vecs_normed,epa_vecs_normed.T),-1.0, 1.0)) / np.pi #calculates the angle\n",
    "#in degrees between each pair of articles\n",
    "epa_article_distances = pd.DataFrame(distances, index=epa_entries_with_words, columns=epa_entries_with_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epa_entries_with_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetMostSimilar(article_id, numrecords=25):\n",
    "    '''Given an article ID, return a list of the most similar (i.e., lowest angle) articles in the dataset'''\n",
    "    return epa_article_distances[str(article_id)].sort_values()[:numrecords+1]\n",
    "\n",
    "GetMostSimilar(23324667)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distance scaling function, engineered to increase slowly up to 70 and rapidly thereafter\n",
    "x_plot = np.arange(0,90,0.5)\n",
    "y_plot = (100 + x_plot) / (85 - x_plot) \n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_distances = (100 + distances) / (85 - distances)\n",
    "scaled_distances[scaled_distances <= 0] = 400 #sets negative values (i.e., angles above 85) to an arbitrarily large distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "topic_DBSCAN = DBSCAN(eps=12, min_samples=10, metric='precomputed')\n",
    "topic_labels = topic_DBSCAN.fit_predict(scaled_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CharacterizeClassPop(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), determine how many\n",
    "    non-noise clusters (i.e., some number besides -1) were assigned, what fraction of articles were assigned to a\n",
    "    non-noise cluster, and the average, median, and max size of the population of clusters. Returns a dict.'''\n",
    "    classCounts = np.array(np.unique(topic_labels, return_counts=True)).T #each row contains the class label and the number of occurrences\n",
    "    assignedClasses = classCounts[1:]\n",
    "    numClust = assignedClasses.shape[0]\n",
    "    fracInClust = np.sum(assignedClasses[:,1]) / np.sum(classCounts[:,1])\n",
    "    avgClustSize = np.sum(assignedClasses[:,1]) / numClust\n",
    "    medianClustSize = np.median(assignedClasses[:,1])\n",
    "    maxClustSize = np.max(assignedClasses[:,1])\n",
    "    \n",
    "    return {'numClusters':numClust,\n",
    "           'coverage':fracInClust,\n",
    "           'avgClusterSize':avgClustSize,\n",
    "           'medianClusterSize':medianClustSize,\n",
    "           'maxClusterSize':maxClustSize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def InfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), select <= 10 representative\n",
    "    articles from each class, and the 10 most frequently present words from that cluster. Returns a nested dict.'''\n",
    "    id_topic = pd.DataFrame(topic_labels, index=epa_entries_with_words, columns=['DB_class'])\n",
    "    vec_words_by_class = pd.merge(epa_vec_count, id_topic,\n",
    "                                 how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    id_topic.index = id_topic.index.map(int)\n",
    "    titles_by_class = pd.merge(epa_titles, id_topic, \n",
    "                               how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "    classInfo = dict()\n",
    "    for k in np.unique(topic_labels)[1:].tolist():\n",
    "        classInfo[k] = dict()\n",
    "        k_words = vec_words_by_class[vec_words_by_class['DB_class'] == k]\n",
    "        classMem = k_words.shape[0]\n",
    "        classInfo[k]['class_members'] = classMem\n",
    "        present = k_words > np.zeros(k_words.shape)\n",
    "        present_by_word = np.sum(present, axis=0)\n",
    "        present_by_word_frac = present_by_word / k_words.shape[0]\n",
    "        classInfo[k]['top_words'] = present_by_word_frac.sort_values(ascending=False)[:10].to_dict()\n",
    "        \n",
    "        sampSize = min(classMem, 10)\n",
    "        k_titles = titles_by_class[titles_by_class['DB_class'] == k].sample(n=sampSize)\n",
    "        classInfo[k]['rep_titles'] = k_titles['page_title'].tolist()\n",
    "    return classInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PrintInfoAboutEachClass(topic_labels):\n",
    "    '''Given an array of class labels (the output from DBSCAN.fit_predict()), run InfoAboutEachClass() and format\n",
    "    its output for printing. Returns a formatted string.'''\n",
    "    \n",
    "    printInfo = InfoAboutEachClass(topic_labels)\n",
    "    report = \"\"\n",
    "    for key in sorted(printInfo):\n",
    "        report += \"========================\\n\"\n",
    "        report += \"CLUSTER {}: {} MEMBERS\\n\\n\".format(key, printInfo[key]['class_members'])\n",
    "        \n",
    "        report += \"Representative articles:\\n\"\n",
    "        for article in printInfo[key]['rep_titles']:\n",
    "            report += article\n",
    "            report += \"\\n\"\n",
    "        report += \"\\nMost common terms:\\n\"\n",
    "        for word in sorted(printInfo[key]['top_words'], key=printInfo[key]['top_words'].get, reverse=True):\n",
    "            report += \"{}: {:.3f}\\n\".format(word, printInfo[key]['top_words'][word])\n",
    "        report += \"========================\\n\"\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RunAndAnalyzeDBSCAN(eps, min_samples, scaled_distances):\n",
    "    '''Given a value for epsilon, the minimum number of samples required for a core point, and a matrix of\n",
    "    precomputed distances, create a DBSCAN classifier, use it to fit and predict based on the supplied distances, \n",
    "    and analyze the output using CharacterizeClassPop and PrintInfoAboutEachClass. Returns a csv formatted string\n",
    "    containing epsilon, min_samples, numClusters, coverage, avgClusterSize, medianClusterSize, and maxClusterSize'''\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clust = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    topic_labels = clust.fit_predict(scaled_distances)\n",
    "    numClusters = np.unique(topic_labels).shape[0]\n",
    "    if numClusters > 1:\n",
    "        cp = CharacterizeClassPop(topic_labels)\n",
    "        report = PrintInfoAboutEachClass(topic_labels)\n",
    "    \n",
    "        reportFN = \"candidate_clusters/{}_{}_clustReport.txt\".format(eps, min_samples)\n",
    "        with open(reportFN, \"w\") as f:\n",
    "            f.write(report)\n",
    "    \n",
    "        return \"{},{},{},{:.3f},{:.0f},{:.0f},{}\".format(eps,min_samples,cp['numClusters'],cp['coverage'],cp['avgClusterSize'],\n",
    "                                        cp['medianClusterSize'],cp['maxClusterSize'])\n",
    "    else:\n",
    "        return \"{},{},0,0,0,0,0\".format(eps,min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_eps = range(2,13)\n",
    "candidate_samples = [2,5,10,20,50,100]\n",
    "with open(\"DBSCAN_metrics.txt\",\"w\") as d:\n",
    "    d.write(\"epsilon,min_samples,num_clusters,coverage,avg_cluster_size,median_cluster_size,max_cluster_size\\n\")\n",
    "    for eps in candidate_eps:\n",
    "        for samples in candidate_samples:\n",
    "            output = RunAndAnalyzeDBSCAN(eps, samples, scaled_distances)\n",
    "            d.write(output)\n",
    "            d.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
